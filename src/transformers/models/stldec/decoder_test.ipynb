{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a7608-45d8-4ac1-bf7d-e8c07b51d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import STLEncoder\n",
    "from handcoded_tokenizer import STLTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba17d9-28e5-42dd-90ea-bd3a0c495b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formulae_to_embed = [\n",
    "    'not ( x_1 <= 0.0956 )', \n",
    "    'not ( x_2 >= 1.118 )', \n",
    "    'not ( ( not ( x_0 <= -0.692 ) and ( eventually[8,19] ( x_2 <= -1.5116 ) until[6,inf] x_2 >= -0.3382 ) ) )', \n",
    "    '( ( x_2 >= -0.4612 or x_1 <= -1.1656 ) or x_0 <= -0.8679 )']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293de6a6-5cf6-4f7b-8a2d-1994f3d81194",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = STLEncoder(embed_dim=10, anchor_filename='anchor_set_10_dim.pickle')\n",
    "formulae_embeddings = encoder.compute_embeddings(formulae_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9279dcd-9d8a-41ca-97f6-564ef86221a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'not ( x_1 >= 0.0956 )'\n",
    "tokenizer = STLTokenizer('tokenizer_files/tokenizer.json')\n",
    "tokenizer.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d669f07f-f687-4920-8a42-ccc60455c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c004db2-74fa-41be-a4c4-fc0b3ff3d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STLAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention as depicted from 'Attention is all you need' \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0, \n",
    "                 is_decoder: bool = False, bias: bool = False, is_causal: bool = False,):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim # overall embedding dimension -> to be divided between multiple heads\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert (self.head_dim * num_heads) == self.embed_dim \n",
    "        self.scaling = self.head_dim ** -0.5 # used to normalized values when projected using `W_` matrices\n",
    "        self.is_decoder = is_decoder # NOT USED\n",
    "        self.is_causal = is_causal # NOT USED\n",
    "\n",
    "        # 'roleplaying' matrices \n",
    "        # note: `embed_dim` refers to the overall number of embedding dimensions, BEFORE splitting them between the heads\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias = bias) \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias = bias)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias = bias)\n",
    "\n",
    "        # to project the heads' outputs into a single vector!\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim, bias = bias) \n",
    "\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, batch_size: int):\n",
    "        \"\"\" \n",
    "        Reshapes tensors to split the input (of dimension `embed_dim`) between multiple heads (of dimension `head_dim`) \n",
    "            Input: (batch_size, seq_len, embed_dim)\n",
    "            Output: (batch_size, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        # `batch_size`` = number of sequences processed in parallel\n",
    "        # `seq_len` = length of each sequence \n",
    "        # `num_heads`, `head_dim` = number of heads of the multi-attn mechanism and dimension of each of them\n",
    "        # `.transpose(1, 2)` swaps the 2nd and the 3rd element\n",
    "        # `.contiguous()` just asks to sotre the data in a contiguous block of memory\n",
    "        return tensor.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "    \n",
    "    \n",
    "    def forward(self, \n",
    "                hidden_states: torch.Tensor, # previous values, passed to the multi-head attn layer\n",
    "                key_value_states: Optional[torch.Tensor] = None, # different key, value items (used in cross-attn)\n",
    "                past_key_value: Optional[Tuple[torch.Tensor]] = None, # stores the key and values of previous steps \n",
    "                attention_mask: Optional[torch.Tensor] = None, # masks non-allowed items (padded or future ones)\n",
    "                layer_head_mask: Optional[torch.Tensor] = None, # used to de-activate specific attn heads (?)\n",
    "                output_attentions: bool = False # flag to control the output of the attn values\n",
    "                ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        \n",
    "        \n",
    "        # if `key_value_states` is provided (i.e. is not None), then `is_cross_attention` is set to True:\n",
    "        is_cross_attention = key_value_states is not None # imagine this as a 'context definition' step\n",
    "\n",
    "\n",
    "        # `hidden_states` has dimensions `(batch_size, tgt_len, embed_dim)`, where:\n",
    "        #   `batch_size` = number of items that are processed simultaneously\n",
    "        #   `tgt_len` = number of tokens in the TARGET sequence\n",
    "        #   `embed_dim` = embedding dimension (per token!)\n",
    "        # batch_size, tgt_len = _ = hidden_states.size()\n",
    "        batch_size, tgt_len, embed_dim = hidden_states.size()\n",
    "\n",
    "        # project the current input in the `query` role:\n",
    "        query = self.W_q(hidden_states) * self.scaling\n",
    "\n",
    "        # to get `key` and `value` we have to differentiate the definition wrt the scenario: \n",
    "\n",
    "        # if we are using cross-attn AND there exist past key values AND the lengths match, then use them!\n",
    "        if (is_cross_attention and past_key_value is not None and past_key_value[0].shape[2] == key_value_states.shape[1]):\n",
    "            # then re-use K and V:\n",
    "            key = past_key_value[0]\n",
    "            value = past_key_value[1]\n",
    "\n",
    "        # if we are using cross-attn BUT we do not have past key values, we compute them\n",
    "        elif is_cross_attention:\n",
    "            key = self._shape(self.W_k(key_value_states), -1, batch_size)\n",
    "            value = self._shape(self.W_v(key_value_states), -1, batch_size)\n",
    "        \n",
    "        # if we do have past key values BUT we are not using cross-attn, then we project the current hidden state to new key and value items\n",
    "        # and after that, concatenate them:\n",
    "        elif past_key_value is not None:\n",
    "            key = self._shape(self.W_k(hidden_states), -1, batch_size)\n",
    "            value = self._shape(self.W_v(hidden_states), -1, batch_size)\n",
    "            key = torch.cat([past_key_value[0], key], dim = 2)\n",
    "            value = torch.cat([past_key_value[1], key], dim = 2)\n",
    "\n",
    "        # if we are in a scenario in which we do not have anything pre-computed, compute them\n",
    "        else:\n",
    "            key = self._shape(self.W_k(hidden_states), -1, batch_size)\n",
    "            value = self._shape(self.W_v(hidden_states), -1, batch_size)\n",
    "\n",
    "\n",
    "        if self.is_decoder:\n",
    "            past_key_value = (key, value)\n",
    "        \n",
    "        \n",
    "        # final shape that we want the queries, keys and values to have BEFORE the attn computation\n",
    "        # we do this because we want each head to be able to operate independently of the others on its set of params\n",
    "        proj_shape = (batch_size * self.num_heads, -1, self.head_dim) # in fact, `head_dim` is the input dimension\n",
    "\n",
    "        # projection: \n",
    "        query = self._shape(query, tgt_len, batch_size).view(*proj_shape)  # `._shape` prepares the vector for the proj\n",
    "        key = key.reshape(*proj_shape)\n",
    "        value = value.reshape(*proj_shape)\n",
    "\n",
    "        src_len = key.size(1) \n",
    "\n",
    "        # bmm = batch(-wise) matrix multiplication between `query` and (TRANSPOSED) `key` \n",
    "        attn_weights = torch.bmm(query, key.transpose(1, 2))\n",
    "\n",
    "        # check n°1\n",
    "        if attn_weights.size() != (batch_size * self.num_heads, tgt_len, src_len):\n",
    "            raise ValueError(f\"Attention weights should be of size {(batch_size * self.num_heads, tgt_len, src_len)}, but is\"\n",
    "             f\" {attn_weights.size()}\"\n",
    "            )\n",
    "        # check n°2\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (batch_size, 1, tgt_len, src_len):\n",
    "                raise ValueError(\n",
    "                f\"Attention mask should be of size {(batch_size, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            # reshaping and application of the mask to the the upper-right part of `attn_weights` matrix\n",
    "            attn_weights = attn_weights.view(batch_size, self.num_heads, tgt_len, src_len) + attention_mask\n",
    "            # returned packed together, not divided among `num_heads` heads\n",
    "            attn_weights = attn_weights.view(batch_size * self.num_heads, tgt_len, src_len)\n",
    "        \n",
    "        # if it passed the checks, then normalize these values on the `key` axis (i.e. `dim = -1`)\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim = -1)\n",
    "\n",
    "        # `layer_head_mask` = 1D array of dimension `self.num_heads` that flags whether or not each head is activated\n",
    "        # e.g. (0, 1, ..., 0.5) -> 1st head deactivated, 2nd activated, ..., last head partially activated (reduced contribution)\n",
    "        if layer_head_mask is not None:\n",
    "            # check \n",
    "            if layer_head_mask.size() != (self.num_heads, ):\n",
    "                raise ValueError(f\"Head mask for a single layer should be of size {(self.num_heads, )}, but is\"\n",
    "                                 f\"{layer_head_mask.size()}\"\n",
    "                                 )\n",
    "            # reshapes the mask from `(self.num_head, )` to `(1, self.num_heads, 1, 1)` to allow the broadcasting\n",
    "            # also attn_weights is again reshaped, splitting for different heads\n",
    "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(batch_size, self.num_heads, tgt_len, src_len)\n",
    "            # final reshape (aka merged again)\n",
    "            attn_weights = attn_weights.view(batch_size * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "\n",
    "        # if this component is asked to output the attention values, then reshape and go back (?)\n",
    "        if output_attentions:\n",
    "            attn_weights_reshaped = attn_weights.view(batch_size, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights_reshaped.view(batch_size * self.num_heads, tgt_len, src_len)\n",
    "        else:\n",
    "            attn_weights_reshaped = None\n",
    "\n",
    "\n",
    "        # apply the dropout to the `attn_weights`:\n",
    "        attn_probs = nn.functional.dropout(attn_weights, p = self.dropout, training = self.training)\n",
    "        # batch-wise matrix multiplication between the resulting probs and the value \n",
    "        attn_output = torch.bmm(attn_probs, value)\n",
    "        # check\n",
    "        if attn_output.size() != (batch_size * self.num_heads, tgt_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(batch_size * self.num_heads, tgt_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "        \n",
    "        \n",
    "        attn_output = attn_output.view(batch_size, self.num_heads, tgt_len, self.head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "\n",
    "\n",
    "        attn_output = attn_output.reshape(batch_size, tgt_len, self.embed_dim)\n",
    "        attn_output = self.W_o(attn_output) \n",
    "\n",
    "        return attn_output, attn_weights_reshaped, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4026494-c615-4f73-944d-31dd061a2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->Marian, BART->MARIAN\n",
    "class MarianDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim: int, \n",
    "                num_decoder_attention_heads: int,\n",
    "                num_decoder_ffn_dim: int,\n",
    "                dropout: float = 0.0,\n",
    "                attention_dropout: float = 0.0,\n",
    "                activation_dropout: float = 0.0,\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # first block\n",
    "        self.self_attn = STLAttention(\n",
    "            embed_dim=self.embed_dim, \n",
    "            num_heads=num_decoder_attention_heads,\n",
    "            dropout=dropout,\n",
    "            is_decoder=True, # not used\n",
    "            is_causal=True, # not used\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.functional.gelu\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # second block\n",
    "        self.encoder_attn = STLAttention(\n",
    "            self.embed_dim,\n",
    "            num_decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            is_decoder=True, # not used\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # third block\n",
    "        self.fc1 = nn.Linear(self.embed_dim, num_decoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(num_decoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
    "                size `(decoder_attention_heads,)`.\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        \n",
    "        ###################################################################\n",
    "        \n",
    "        # BLOCK 1: processing what has been previously generated \n",
    "\n",
    "        # previous state is stored into an auxiliary variable `residual`\n",
    "        residual = hidden_states\n",
    "\n",
    "        # tries to exploit previous K, V values if there are any \n",
    "        # (practically picks up to the first 2 values stored in `past_key_value` vector)\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "\n",
    "        # masked MHSA on the already generated sequence\n",
    "        # invokes `forward` method to transform the original vector accordingly \n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn.forward(\n",
    "            hidden_states=hidden_states, # Q\n",
    "            past_key_value=self_attn_past_key_value, # K, V\n",
    "            attention_mask=attention_mask, # passed as input of the decoder layer\n",
    "            layer_head_mask=layer_head_mask, # to deactivate certain attn layers \n",
    "            output_attentions=output_attentions, \n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # normalization\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        ###################################################################\n",
    "\n",
    "        # BLOCK 2: cross-attn between already generated input and previous information (from the encoder)\n",
    "\n",
    "        # initialize K, Q, attn_weights for this new attn operation\n",
    "        cross_attn_present_key_value = None \n",
    "        cross_attn_weights = None\n",
    "\n",
    "        # the important condition is that the encoder carries some information\n",
    "        if encoder_hidden_states is not None:\n",
    "\n",
    "            # previous state is stored into an auxiliary variable `residual`\n",
    "            residual = hidden_states\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3, 4 of PAST_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "\n",
    "            # MHSA in cross-attn\n",
    "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn.forward(\n",
    "                hidden_states=hidden_states, # Q = generated output\n",
    "                key_value_states=encoder_hidden_states, # K, V = encoder memory (used only in the 1st step when `use_cache = True`)\n",
    "                layer_head_mask=cross_attn_layer_head_mask, # again to mask certain heads\n",
    "                past_key_value=cross_attn_past_key_value, # K, V = encoder CACHED memory (used from the 2nd step on when `use_cache = True`)\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "            # residual connection\n",
    "            hidden_states = residual + hidden_states\n",
    "\n",
    "            # normalization\n",
    "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3, 4 of PRESENT_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        ###################################################################\n",
    "\n",
    "        # BLOCK 3: FFNN (transforming some merged generated output - encoder information)\n",
    "\n",
    "        # previous state is stored into an auxiliary variable `residual`\n",
    "        residual = hidden_states\n",
    "\n",
    "        # FFNN - core\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # normalization\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights, cross_attn_weights)\n",
    "\n",
    "        if use_cache: # if not, picks again K and V each time\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9bec3-8939-4d60-a067-330233715b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assicurati che la classe STLAttention sia già definita nel tuo codice.\n",
    "\n",
    "# Parametri di esempio\n",
    "embed_dim = 16  # dimensione dell'embedding\n",
    "num_heads = 4   # numero di \"head\" nell'attenzione multi-head\n",
    "dropout = 0.1   # probabilità di dropout\n",
    "\n",
    "# Crea un'istanza del modello STLAttention\n",
    "attention_layer = STLAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "# Definisci gli input di esempio\n",
    "\n",
    "# Dimensioni: (batch_size, seq_len, embed_dim)\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "# hidden_states: rappresentano gli stati nascosti da passare nell'attenzione\n",
    "hidden_states = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# key_value_states (solo se stai usando cross-attention; se no, puoi passare None)\n",
    "key_value_states = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# attention_mask: se vogliamo mascherare alcune posizioni (ad esempio, per il padding)\n",
    "attention_mask = torch.zeros(batch_size, 1, seq_len, seq_len)  # Una maschera per tutte le posizioni\n",
    "\n",
    "# layer_head_mask: attiva o disattiva determinati \"head\" dell'attenzione\n",
    "layer_head_mask = torch.ones(num_heads)  # Attiva tutti i \"head\" dell'attenzione\n",
    "\n",
    "# Esegui il forward pass\n",
    "attn_output, attn_weights, past_key_value = attention_layer.forward(\n",
    "    hidden_states=hidden_states,\n",
    "    key_value_states=key_value_states,\n",
    "    attention_mask=attention_mask,\n",
    "    layer_head_mask=layer_head_mask,\n",
    "    output_attentions=True  # Includi i pesi di attenzione nell'output\n",
    ")\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"Attn Output:\")\n",
    "print(attn_output.shape)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "print(\"\\nAttn Weights:\")\n",
    "print(attn_weights.shape if attn_weights is not None else \"No attention weights\")\n",
    "\n",
    "print(\"\\nPast Key Values:\")\n",
    "print(past_key_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982c210-ef7c-40a6-b246-ab909a189b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MarianDecoderLayer(10, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a916bb0-6788-4654-a587-1ba4d4774b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff5369-0614-4bc7-b429-71da1e3b1c63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
