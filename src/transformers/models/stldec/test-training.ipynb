{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ed1d23-1008-4ab7-bfbb-eaebf77c255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ed27ab-3eb9-4ea5-9bda-4f99b07e5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import get_scheduler\n",
    "from accelerate import Accelerator\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import chain\n",
    "from datasets.utils.logging import set_verbosity_warning, set_verbosity_info\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ef70ed-ad81-44f4-a910-3fab9ccf1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration variables for Jupyter notebook\n",
    "args = {\n",
    "    'dataset_name': None,  # or a custom dataset path\n",
    "    'train_file': None,\n",
    "    'validation_file': None,\n",
    "    'output_dir': './output',\n",
    "    'model_name_or_path': 'STLForCausalLM',\n",
    "    'tokenizer_name': 'STLTokenizer',\n",
    "    'block_size': 128,\n",
    "    'batch_size': 8,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 3,\n",
    "    'learning_rate': 5e-5,\n",
    "    'weight_decay': 0.01,\n",
    "    'num_warmup_steps': 0,\n",
    "    'max_train_steps': None,\n",
    "    'seed': 42,\n",
    "    'with_tracking': False,\n",
    "    'hub_model_id': None,\n",
    "    'push_to_hub': False,\n",
    "    'trust_remote_code': False,\n",
    "    'overwrite_cache': False,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8,\n",
    "    'checkpointing_steps': 'epoch',  # or 'steps' with an int value\n",
    "    'resume_from_checkpoint': None,\n",
    "}\n",
    "\n",
    "# Initialize the accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Send telemetry for resource tracking (assuming you have this function)\n",
    "# send_example_telemetry(\"run_clm_no_trainer\", args)\n",
    "\n",
    "# Set seed\n",
    "if args['seed'] is not None:\n",
    "    torch.manual_seed(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dfa915d-57d7-413d-8c27-5a4f2fc30549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets\n",
    "if args['dataset_name']:\n",
    "    raw_datasets = load_dataset(args['dataset_name'])\n",
    "else:\n",
    "    data_files = {}\n",
    "    if args['train_file']:\n",
    "        data_files[\"train\"] = args['train_file']\n",
    "    if args['validation_file']:\n",
    "        data_files[\"validation\"] = args['validation_file']\n",
    "    raw_datasets = load_dataset('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac6ec27-6ba7-4206-91c6-9916e350efdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /u/dssc/scandu00/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Inserisci il tuo token Hugging Face qui\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424aa07c-9930-4096-901c-8cf71b9cf2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d6783-7407-46a2-9d62-19c1f05f7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "block_size = args['block_size']\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "\n",
    "train_dataset = lm_datasets['train']\n",
    "eval_dataset = lm_datasets['validation']\n",
    "\n",
    "# DataLoader creation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args['per_device_train_batch_size'], shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=args['per_device_eval_batch_size'])\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args['learning_rate'], weight_decay=args['weight_decay'])\n",
    "\n",
    "# Scheduler setup\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args['gradient_accumulation_steps'])\n",
    "if args['max_train_steps'] is None:\n",
    "    args['max_train_steps'] = args['num_train_epochs'] * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear', optimizer=optimizer, num_warmup_steps=args['num_warmup_steps'],\n",
    "    num_training_steps=args['max_train_steps']\n",
    ")\n",
    "\n",
    "# Prepare everything with accelerator\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(args['num_train_epochs']):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.set_postfix(loss=total_loss / (step + 1))\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            eval_loss += outputs.loss.item()\n",
    "\n",
    "    eval_loss /= len(eval_dataloader)\n",
    "    perplexity = math.exp(eval_loss) if eval_loss < 100 else float('inf')\n",
    "    print(f\"Epoch {epoch} evaluation loss: {eval_loss}, perplexity: {perplexity}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    if args['checkpointing_steps'] == 'epoch' and (epoch + 1) % 1 == 0:\n",
    "        output_dir = os.path.join(args['output_dir'], f\"epoch_{epoch}\")\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Final model saving\n",
    "model.save_pretrained(args['output_dir'])\n",
    "tokenizer.save_pretrained(args['output_dir'])\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
