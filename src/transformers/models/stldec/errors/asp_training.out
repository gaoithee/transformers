Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
02/15/2025 17:51:13 - INFO - __main__ - ***** Running training *****
02/15/2025 17:51:13 - INFO - __main__ -   Num examples = 78773
02/15/2025 17:51:13 - INFO - __main__ -   Num Epochs = 10
02/15/2025 17:51:13 - INFO - __main__ -   Instantaneous batch size per device = 64
02/15/2025 17:51:13 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64
02/15/2025 17:51:13 - INFO - __main__ -   Gradient Accumulation steps = 1
02/15/2025 17:51:13 - INFO - __main__ -   Max optimization steps = 50000
  0%|          | 0/50000 [00:00<?, ?it/s]02/15/2025 17:51:13 - INFO - accelerate.accelerator - Loading states from tf_output_test_16batch/step_20000
02/15/2025 17:51:15 - INFO - accelerate.checkpointing - All model weights loaded successfully
02/15/2025 17:51:16 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
02/15/2025 17:51:16 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
02/15/2025 17:51:16 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
02/15/2025 17:51:16 - INFO - accelerate.checkpointing - All random states loaded successfully
02/15/2025 17:51:16 - INFO - accelerate.accelerator - Loading in 0 custom states
02/15/2025 17:51:16 - INFO - __main__ - Starting epoch = 7, resume step = 20000
02/15/2025 17:51:16 - INFO - __main__ - Resuming training from the specified step
02/15/2025 17:51:16 - INFO - __main__ - Total expected steps: 21580
02/15/2025 17:52:00 - INFO - __main__ -   Loss = 0.13293462991714478, epoch = 8, step = 304
02/15/2025 17:52:00 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:52:00 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:52:02 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:52:03 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:52:03 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:52:03 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:52:03 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:52:03 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:52:24 - INFO - __main__ -   Loss = 0.1459471732378006, epoch = 8, step = 305
02/15/2025 17:52:24 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:52:24 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:52:24 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:52:26 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:52:26 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:52:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:52:26 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:52:26 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:52:47 - INFO - __main__ -   Loss = 0.19058646261692047, epoch = 8, step = 306
02/15/2025 17:52:47 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:52:47 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:52:48 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:52:49 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:52:49 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:52:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:52:49 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:52:49 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:53:10 - INFO - __main__ -   Loss = 0.1300979107618332, epoch = 8, step = 307
02/15/2025 17:53:10 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:53:10 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:53:11 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:53:13 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:53:13 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:53:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:53:13 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:53:13 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:53:33 - INFO - __main__ -   Loss = 0.10409903526306152, epoch = 8, step = 308
02/15/2025 17:53:33 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:53:33 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:53:34 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:53:35 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:53:35 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:53:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:53:35 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:53:35 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:53:56 - INFO - __main__ -   Loss = 0.12887528538703918, epoch = 8, step = 309
02/15/2025 17:53:56 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:53:56 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:53:57 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:53:58 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:53:58 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:53:58 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:53:58 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:53:58 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:54:19 - INFO - __main__ -   Loss = 0.1398610919713974, epoch = 8, step = 310
02/15/2025 17:54:19 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:54:19 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:54:20 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:54:22 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:54:22 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:54:22 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:54:22 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:54:22 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:54:42 - INFO - __main__ -   Loss = 0.10032468289136887, epoch = 8, step = 311
02/15/2025 17:54:42 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:54:42 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:54:43 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:54:45 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:54:45 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:54:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:54:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:54:45 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:55:05 - INFO - __main__ -   Loss = 0.1525512933731079, epoch = 8, step = 312
02/15/2025 17:55:05 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:55:05 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:55:06 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:55:08 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:55:08 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:55:08 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:55:08 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:55:08 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:55:29 - INFO - __main__ -   Loss = 0.10449456423521042, epoch = 8, step = 313
02/15/2025 17:55:29 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:55:29 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:55:29 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:55:31 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:55:31 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:55:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:55:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:55:31 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:55:51 - INFO - __main__ -   Loss = 0.12253963947296143, epoch = 8, step = 314
02/15/2025 17:55:51 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:55:51 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:55:52 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:55:54 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:55:54 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:55:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:55:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:55:54 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:56:14 - INFO - __main__ -   Loss = 0.11639906466007233, epoch = 8, step = 315
02/15/2025 17:56:14 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:56:14 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:56:15 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:56:16 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:56:16 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:56:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:56:16 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:56:16 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:56:37 - INFO - __main__ -   Loss = 0.14806288480758667, epoch = 8, step = 316
02/15/2025 17:56:37 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:56:37 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:56:38 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:56:39 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:56:39 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:56:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:56:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:56:39 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:57:00 - INFO - __main__ -   Loss = 0.13909445703029633, epoch = 8, step = 317
02/15/2025 17:57:00 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:57:00 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:57:01 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:57:02 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:57:02 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:57:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:57:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:57:02 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:57:23 - INFO - __main__ -   Loss = 0.09634341299533844, epoch = 8, step = 318
02/15/2025 17:57:23 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:57:23 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:57:24 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:57:25 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:57:25 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:57:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:57:25 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:57:25 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:57:46 - INFO - __main__ -   Loss = 0.13795004785060883, epoch = 8, step = 319
02/15/2025 17:57:46 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:57:46 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:57:46 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:57:48 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:57:48 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:57:48 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:57:48 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:57:48 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:58:08 - INFO - __main__ -   Loss = 0.13551495969295502, epoch = 8, step = 320
02/15/2025 17:58:08 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:58:08 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:58:09 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:58:11 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:58:11 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:58:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:58:11 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:58:11 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:58:31 - INFO - __main__ -   Loss = 0.10379799455404282, epoch = 8, step = 321
02/15/2025 17:58:31 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:58:31 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:58:32 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:58:34 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:58:34 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:58:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:58:34 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:58:34 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:58:55 - INFO - __main__ -   Loss = 0.11439355462789536, epoch = 8, step = 322
02/15/2025 17:58:55 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:58:55 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:58:55 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:58:57 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:58:57 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:58:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:58:57 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:58:57 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:59:18 - INFO - __main__ -   Loss = 0.17481334507465363, epoch = 8, step = 323
02/15/2025 17:59:18 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:59:18 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:59:19 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:59:21 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:59:21 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:59:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:59:21 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:59:21 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 17:59:42 - INFO - __main__ -   Loss = 0.18176044523715973, epoch = 8, step = 324
02/15/2025 17:59:42 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 17:59:42 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 17:59:43 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 17:59:45 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 17:59:45 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 17:59:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 17:59:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 17:59:45 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:00:06 - INFO - __main__ -   Loss = 0.1260198950767517, epoch = 8, step = 325
02/15/2025 18:00:06 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:00:06 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:00:07 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:00:08 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:00:08 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:00:08 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:00:08 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:00:08 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:00:30 - INFO - __main__ -   Loss = 0.13611489534378052, epoch = 8, step = 326
02/15/2025 18:00:30 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:00:30 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:00:31 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:00:32 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:00:32 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:00:32 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:00:32 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:00:32 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:00:54 - INFO - __main__ -   Loss = 0.1076376736164093, epoch = 8, step = 327
02/15/2025 18:00:54 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:00:54 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:00:54 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:00:56 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:00:56 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:00:56 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:00:56 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:00:56 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:01:18 - INFO - __main__ -   Loss = 0.1363946944475174, epoch = 8, step = 328
02/15/2025 18:01:18 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:01:18 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:01:18 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:01:20 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:01:20 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:01:20 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:01:20 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:01:20 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:01:41 - INFO - __main__ -   Loss = 0.14179116487503052, epoch = 8, step = 329
02/15/2025 18:01:41 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:01:41 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:01:42 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:01:44 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:01:44 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:01:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:01:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:01:44 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:02:05 - INFO - __main__ -   Loss = 0.10941162705421448, epoch = 8, step = 330
02/15/2025 18:02:05 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:02:05 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:02:05 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:02:07 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:02:07 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:02:07 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:02:07 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:02:07 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:02:28 - INFO - __main__ -   Loss = 0.11127782613039017, epoch = 8, step = 331
02/15/2025 18:02:28 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:02:28 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:02:29 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:02:31 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:02:31 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:02:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:02:31 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:02:31 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:02:52 - INFO - __main__ -   Loss = 0.15371708571910858, epoch = 8, step = 332
02/15/2025 18:02:52 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:02:52 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:02:53 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:02:54 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:02:54 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:02:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:02:54 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:02:54 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:03:15 - INFO - __main__ -   Loss = 0.16155706346035004, epoch = 8, step = 333
02/15/2025 18:03:15 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:03:15 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:03:16 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:03:18 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:03:18 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:03:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:03:18 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:03:18 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
02/15/2025 18:03:39 - INFO - __main__ -   Loss = 0.15416759252548218, epoch = 8, step = 334
02/15/2025 18:03:39 - INFO - accelerate.accelerator - Saving current state to step_20000/step_20000
02/15/2025 18:03:39 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/15/2025 18:03:40 - INFO - accelerate.checkpointing - Model weights saved in step_20000/step_20000/model.safetensors
02/15/2025 18:03:41 - INFO - accelerate.checkpointing - Optimizer state saved in step_20000/step_20000/optimizer.bin
02/15/2025 18:03:41 - INFO - accelerate.checkpointing - Scheduler state saved in step_20000/step_20000/scheduler.bin
02/15/2025 18:03:41 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in step_20000/step_20000/sampler.bin
02/15/2025 18:03:41 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in step_20000/step_20000/sampler_1.bin
02/15/2025 18:03:41 - INFO - accelerate.checkpointing - Random states saved in step_20000/step_20000/random_states_0.pkl
srun: Job step aborted: Waiting up to 92 seconds for job step to finish.
slurmstepd: error: *** STEP 12548823.0 ON lrdn2081 CANCELLED AT 2025-02-15T18:04:00 ***
slurmstepd: error: *** JOB 12548823 ON lrdn2081 CANCELLED AT 2025-02-15T18:04:00 ***
