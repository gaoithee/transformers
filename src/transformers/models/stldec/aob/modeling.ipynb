{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3e4aca2-45bf-4182-922d-4adc8d4a4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2afedcc-9e91-45c5-81d1-ca4afc6f7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import (\n",
    "    add_end_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d236aef-37c6-45dd-9ea3-e1317f0fde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf9a7fc-6523-4be8-a6eb-fdd34b8b2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import STLPreTrainedModel\n",
    "from utils2 import STLConfig\n",
    "from encoder import STLEncoder\n",
    "from decoder import STLDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a895bbbf-e88c-4191-8b12-936e08e6b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CONFIG_FOR_DOC = \"STLConfig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7f3d24a-93db-45f3-a3c2-81be53b20a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarianModel(STLPreTrainedModel):\n",
    "    _tied_weights_keys = [\"decoder.embed_tokens.weight\"]\n",
    "\n",
    "    def __init__(self, config: STLConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
    "\n",
    "        # Embedding condiviso solo per il decoder\n",
    "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
    "        if self.config.share_encoder_decoder_embeddings:\n",
    "            decoder_embed_tokens = self.shared\n",
    "        else:\n",
    "            # Se gli embeddings non sono condivisi, facciamo una copia per il decoder\n",
    "            decoder_embed_tokens = copy.deepcopy(self.shared)\n",
    "            self.shared = None\n",
    "\n",
    "        # Decoder-only: senza encoder\n",
    "        self.decoder = STLDecoder(config)\n",
    "\n",
    "        # Inizializzazione dei pesi\n",
    "        self.post_init()\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "\n",
    "    def resize_decoder_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
    "        # La dimensione del vocabolario del decoder\n",
    "        old_embeddings = self.get_decoder_input_embeddings()\n",
    "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
    "        self.set_decoder_input_embeddings(new_embeddings)\n",
    "\n",
    "        model_embeds = self.get_decoder_input_embeddings()\n",
    "\n",
    "        if new_num_tokens is None:\n",
    "            return model_embeds\n",
    "\n",
    "        # Aggiorna la configurazione del modello\n",
    "        self.config.decoder_vocab_size = new_num_tokens\n",
    "\n",
    "        # Tie weights, se necessario\n",
    "        self.tie_weights()\n",
    "\n",
    "        return model_embeds\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"\n",
    "        Metodo di forward per il modello decoder-only.\n",
    "\n",
    "        In un modello decoder-only, gli `input_ids` vengono utilizzati solo per generare la sequenza,\n",
    "        senza un encoder che fornisca rappresentazioni da un'altra sequenza (come nel caso di traduzione o altre attivitÃ  seq2seq).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Passaggio attraverso il decoder-only\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs\n",
    "\n",
    "        # Restituisci l'output come dizionario\n",
    "        return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e41b4ea1-467d-45e8-964c-850427b09428",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = STLConfig()\n",
    "test = MarianModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f81fcf-f9da-4649-948e-93a4d82f0389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output del modello ottenuto\n"
     ]
    }
   ],
   "source": [
    "# Creazione degli input per il decoder\n",
    "input_ids = torch.randint(0, config.vocab_size, (1, 10))  # Batch di dimensione 1, sequenza di lunghezza 10\n",
    "decoder_input_ids = torch.randint(0, config.vocab_size, (1, 10))  # Sequenza di lunghezza 10 per il decoder\n",
    "\n",
    "# Creazione di una maschera di attenzione\n",
    "attention_mask = torch.ones(1, 10)  # Tutti i token sono validi\n",
    "\n",
    "# Chiamata al metodo forward\n",
    "outputs = test(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    decoder_attention_mask=attention_mask,\n",
    "    return_dict=True  # Restituisce un dizionario\n",
    ")\n",
    "\n",
    "# Visualizzazione dell'output\n",
    "print(\"Output del modello ottenuto\")\n",
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "78f0dddc-3a4f-4efd-a8c9-b487490628de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import GenerationMixin\n",
    "\n",
    "class STLForCausalLM(STLPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        config = copy.deepcopy(config)\n",
    "        config.is_decoder = True\n",
    "        config.is_encoder_decoder = False\n",
    "        \n",
    "        super().__init__(config)\n",
    "        self.model = STLDecoder(config)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def set_decoder(self, decoder):\n",
    "        self.model = decoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            head_mask=head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        logits = self.lm_head(outputs[0])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eac8c59b-97eb-4a9c-bff6-fd3eaabc140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = STLConfig()\n",
    "test = MarianForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "134140da-cea6-4839-88c8-168f801fe049",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "seq_length = 20\n",
    "input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
    "labels = torch.randint(0, config.vocab_size, (batch_size, seq_length))\n",
    "\n",
    "# Run the model\n",
    "outputs = test(input_ids=input_ids, labels=labels, return_dict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ee7aa96-2cb0-4c8e-8bf6-15636863cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5720567-ba4d-435c-858e-cad8a84b1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"STLdec\", STLConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d3355976-e7d6-42f5-b646-63d9141a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModel.register(STLConfig, STLForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df3ea54-8c74-4746-b08b-bf69f71ad8b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
