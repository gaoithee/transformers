{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b680be2-b5f0-4a66-8277-c0e79c9044ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esempi di sequenze\n",
    "sequences = [\n",
    "    '( not ( x_1 <= 0.2988 ) until[11,21] x_0 <= -0.7941 )',\n",
    "    '( eventually[10,21] ( ( x_0 <= 0.9229 or not ( always[5,inf] ( x_0 <= 0.4263 ) ) ) ) or x_0 >= 0.6745 )',\n",
    "    '( ( eventually ( ( x_1 <= 0.5132 and x_0 <= -0.457 ) ) until x_2 <= 0.257 ) and x_1 <= 0.1586 )',\n",
    "    'always[0,3] ( ( always[14,20] ( x_1 >= -0.6583 ) until[12,18] x_1 <= -0.284 ) )',\n",
    "    '( not ( ( ( x_0 <= -1.0103 and eventually[1,6] ( x_1 <= 0.1484 ) ) and always[1,14] ( x_2 <= -1.7752 ) ) ) or not ( x_1 <= 1.0504 ) )'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866ac8f-34ed-4877-a319-e4a82a54bdad",
   "metadata": {},
   "source": [
    "# Da qua roba seria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d7c02-b36b-4944-95b7-56c57cf0095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "# omitted sentencepiece!\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"source_spm\": None,\n",
    "    \"target_spm\": None,\n",
    "    \"vocab\": \"tokenizer_files/tokenizer.json\",\n",
    "    \"target_vocab_file\": None,\n",
    "    \"tokenizer_config_file\": \"tokenizer_files/tokenizer_config.json\",\n",
    "}\n",
    "\n",
    "SPIECE_UNDERLINE = \"▁\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437614d1-3a2a-4f7b-892a-30e479d45a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STLTokenizer(PreTrainedTokenizer):\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: str,\n",
    "        unk_token: str = \"unk\",\n",
    "        eos_token: str = \"/s\",\n",
    "        pad_token: str = \"pad\",\n",
    "        model_max_length: int = 512,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab = self.load_json(vocab)\n",
    "        self.unk_token = unk_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        # capisci meglio qui\n",
    "        self.encoder = self.vocab\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "\n",
    "        # super().__init__(unk_token=unk_token, eos_token=eos_token, pad_token=pad_token, model_max_length=model_max_length, **kwargs)\n",
    "\n",
    "    def load_json(self, path: str) -> Dict:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def preprocess_sequence(self, sequence, pad_token=\"pad\"):\n",
    "        \"\"\"\n",
    "        Sostituisce ogni spazio nella sequenza con un token di padding specificato.\n",
    "        \n",
    "        Args:\n",
    "            sequence (str): La sequenza di input.\n",
    "            pad_token (str): Il token da utilizzare per il padding (default: \"<pad>\").\n",
    "        \n",
    "        Returns:\n",
    "            str: La sequenza preprocessata con gli spazi sostituiti.\n",
    "        \"\"\"\n",
    "        return sequence.replace(' ', f' {pad_token} ')\n",
    "\n",
    "    def split_number(self, number: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Divide un numero in token individuali (es. 2.0375 -> ['2', '.', '0', '3', '7', '5']).\n",
    "        \"\"\"\n",
    "        return list(number)\n",
    "\n",
    "    def split_variable(self, variable: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Divide una variabile come x_0 in token individuali (es. x_0 -> ['x', '_', '0']).\n",
    "        \"\"\"\n",
    "        return list(variable)\n",
    "\n",
    "    def tokenize(self, text: str, vocab: dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizza una stringa basandosi sul vocabolario dato.\n",
    "        \n",
    "        Args:\n",
    "            text (str): La stringa da tokenizzare.\n",
    "            vocab (dict): Il vocabolario che definisce i token validi.\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: Lista di token estratti.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess to replace spaces with pad_token\n",
    "        text = self.preprocess_sequence(text)\n",
    "\n",
    "        # non generalizzabile -> prendi il vocabolario, crea espressioni a partire da questo\n",
    "        # ma come?\n",
    "\n",
    "        # Modified token pattern to capture space-replaced token\n",
    "        token_pattern = re.compile(\n",
    "            r'(always|eventually|until|'  # Temporal operators\n",
    "            r'and|or|not|'  # Logical operators\n",
    "            r'x_\\d+|'  # Variables (e.g., x_0)\n",
    "            r'-?\\d+\\.\\d+|-?\\d+|'  # Numbers (integers or decimals with sign)\n",
    "            r'<=|>=|<|>|=|'  # Relational operators\n",
    "            r'\\[|\\]|\\(|\\)|,|inf|'  # Special symbols\n",
    "            r'pad)'  # Match <pad> as a token (escape < and >)\n",
    "        )\n",
    "        \n",
    "        # Trova tutti i token usando il pattern\n",
    "        tokens = token_pattern.findall(text)\n",
    "        \n",
    "        # Espandi numeri e variabili\n",
    "        expanded_tokens = []\n",
    "        for token in tokens:\n",
    "            if re.match(r\"-?\\d+\\.\\d+|-?\\d+\", token):  # Numeri\n",
    "                expanded_tokens.extend(self.split_number(token))\n",
    "            elif re.match(r\"x_\\d+\", token):  # Variabili\n",
    "                expanded_tokens.extend(self.split_variable(token))\n",
    "            else:  # Altri token validi\n",
    "                expanded_tokens.append(token)\n",
    "        return expanded_tokens\n",
    "\n",
    "    def code(self, tokens):\n",
    "        # Sostituire i token con i rispettivi ID nel vocabolario, usando <unk> per quelli non presenti\n",
    "        return [self.encoder.get(token, self.encoder[self.unk_token]) for token in tokens]\n",
    "\n",
    "    # Funzione di detokenizzazione\n",
    "    def decode(self, token_ids):\n",
    "        tokens = [self.decoder.get(id, self.unk_token) for id in token_ids]\n",
    "        return \"\".join(tokens).replace(f'pad', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cec796-7725-490b-8bf8-21d01278ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = STLTokenizer(vocab = \"tokenizer_files/tokenizer-v2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d456c-c875-4231-bd82-7ef49df0ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580a151-2e76-4dbd-85ba-3cafb35ed526",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tokenizer.tokenize(sequences[0], \"tokenizer_files/tokenizer.json\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638a4a7-f88b-44cc-b411-a4653799a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.code(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a49bd4-3207-48dd-b9fb-692f089deba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ccdbf-6709-47d2-8166-56243978b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c8d48-884d-44ee-a434-4cfd812f9088",
   "metadata": {},
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd7453-3770-4ce5-a41e-37cefa26674c",
   "metadata": {},
   "source": [
    "Cerca di capire se è necessario trasformare l'output della parte di codifica (`code`) e di quello di decodifica (`decode`) in dizionari storati in qualsivoglia `json` file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8687ca-6405-4ca2-bed7-bb3812a84c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Vocabolario\n",
    "vocab = {'unk': 0, 'pad': 1, '/s': 2, 's': 3, '(': 4, ')': 5, 'always': 6, 'eventually': 7, \n",
    "         'until': 8, 'and': 9, 'or': 10, 'not': 11, '>=': 12, '<=': 13, '>': 14, '<': 15, \n",
    "         '=': 16, 'x': 17, '_': 18, '[': 19, ']': 20, ',': 21, 'inf': 22, '-': 23, '.': 24, \n",
    "         '0': 25, '1': 26, '2': 27, '3': 28, '4': 29, '5': 30, '6': 31, '7': 32, '8': 33, '9': 34}\n",
    "\n",
    "def tokenize_vocabulary_based(sequence, vocab):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(sequence):\n",
    "        best_match = None\n",
    "        # Trova la migliore corrispondenza dal vocabolario\n",
    "        for j in range(len(sequence), i, -1):  # Scansione della sequenza di destra verso sinistra\n",
    "            subtoken = sequence[i:j]\n",
    "            if subtoken in vocab:\n",
    "                best_match = subtoken\n",
    "                break\n",
    "        if best_match:\n",
    "            tokens.append(best_match)\n",
    "            i += len(best_match)\n",
    "        else:\n",
    "            tokens.append('unk')  # Se non c'è corrispondenza, usa 'unk'\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "# Sequenza da tokenizzare\n",
    "sequence = \"(padnotpad(padx_1pad<=pad0.2988pad)paduntil[11,21]padx_0pad<=pad-0.7941pad)\"\n",
    "\n",
    "# Tokenizzazione\n",
    "tokenized_sequence = tokenize_vocabulary_based(sequence, vocab)\n",
    "print(\"Tokenized sequence:\", tokenized_sequence)\n",
    "\n",
    "# Mappatura ai relativi ID\n",
    "token_ids = [vocab.get(token, vocab['unk']) for token in tokenized_sequence]\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78fa8a4-9e6c-4073-bb06-3ecc0286c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e869cd3e-9e03-4ec3-9be4-55cbf0e1e63a",
   "metadata": {},
   "source": [
    "# NEW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8c2a56d-c600-4fca-9a47-e7d3e29f72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "def load_json(path: str) -> Union[Dict, List]:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "class STLTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab_path: str, unk_token: str = \"unk\", pad_token: str = \"pad\", bos_token: str = \"/s\", eos_token: str = \"s\"):\n",
    "        self.vocab = load_json(vocab_path)\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}  # Reverse mapping\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def prepad_sequence(self, sequence, undo = False):\n",
    "        \"\"\"\n",
    "        Sostituisce ogni spazio nella sequenza con un token di padding specificato.\n",
    "        \n",
    "        Args:\n",
    "            sequence (str): La sequenza di input.\n",
    "            undo (bool): pad when False, un-pad when True.\n",
    "        \n",
    "        Returns:\n",
    "            str: La sequenza preprocessata con ` `/`pad` sostituiti.\n",
    "        \"\"\"\n",
    "        if undo:\n",
    "            return sequence.replace(f'{self.pad_token}', ' ')\n",
    "        else:\n",
    "            return sequence.replace(' ', f'{self.pad_token}')\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \n",
    "        text = self.prepad_sequence(text)\n",
    "        \n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            best_match = None\n",
    "            for j in range(len(text), i, -1):  \n",
    "                subtoken = text[i:j]\n",
    "                if subtoken in self.vocab:\n",
    "                    best_match = subtoken\n",
    "                    break\n",
    "            if best_match:\n",
    "                tokens.append(best_match)\n",
    "                i += len(best_match)\n",
    "            else:\n",
    "                tokens.append(self.unk_token)\n",
    "                i += 1\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
    "        return [self.id_to_token.get(i, self.unk_token) for i in ids]\n",
    "\n",
    "    def decode(self, token_ids: List[int], skip_special_tokens: bool = False) -> str:\n",
    "        tokens = self.convert_ids_to_tokens(token_ids)\n",
    "        decoded = \"\".join(tokens)\n",
    "        return self.prepad_sequence(decoded, undo = True)\n",
    "\n",
    "    def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]:\n",
    "        # implementato per sentencepiece per salvare il vocabolario trovato -> non serve\n",
    "        vocab_file = f\"{save_directory}/{filename_prefix + '-' if filename_prefix else ''}vocab.json\"\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.vocab, f, indent=2, ensure_ascii=False)\n",
    "        return (vocab_file,)\n",
    "\n",
    "    def get_vocab(self) -> dict:\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "# Esempio di utilizzo\n",
    "# tokenizer = STLTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3f4a1c5-774f-476f-a657-cded937fff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = STLTokenizer('tokenizer_files/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99cb8698-dc90-4288-80f0-7d6e2d526b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequence: ['(', 'pad', 'not', 'pad', '(', 'pad', 'x', '_', '1', 'pad', '<=', 'pad', '0', '.', '2', '9', '8', '8', 'pad', ')', 'pad', 'until', '[', '1', '1', ',', '2', '1', ']', 'pad', 'x', '_', '0', 'pad', '<=', 'pad', '-', '0', '.', '7', '9', '4', '1', 'pad', ')']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizzazione\n",
    "# sequence = \"(padnotpad(padx_1pad<=pad0.2988pad)paduntil[11,21]padx_0pad<=pad-0.7941pad)\"\n",
    "sequence = \"( not ( x_1 <= 0.2988 ) until[11,21] x_0 <= -0.7941 )\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"Tokenized sequence:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "934148fd-e9f2-40da-8a6a-014da75f8cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [4, 1, 11, 1, 4, 1, 17, 18, 26, 1, 13, 1, 25, 24, 27, 34, 33, 33, 1, 5, 1, 8, 19, 26, 26, 21, 27, 26, 20, 1, 17, 18, 25, 1, 13, 1, 23, 25, 24, 32, 34, 29, 26, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21006994-09a5-451e-a75d-faa874a33b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sequence: ( not ( x_1 <= 0.2988 ) until[11,21] x_0 <= -0.7941 )\n"
     ]
    }
   ],
   "source": [
    "decoded_sequence = tokenizer.decode(token_ids)\n",
    "print(\"Decoded sequence:\", decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dff1f25d-a0bd-4353-a8a3-c79183b4f103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk': 0,\n",
       " 'pad': 1,\n",
       " '/s': 2,\n",
       " 's': 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " 'always': 6,\n",
       " 'eventually': 7,\n",
       " 'until': 8,\n",
       " 'and': 9,\n",
       " 'or': 10,\n",
       " 'not': 11,\n",
       " '>=': 12,\n",
       " '<=': 13,\n",
       " '>': 14,\n",
       " '<': 15,\n",
       " '=': 16,\n",
       " 'x': 17,\n",
       " '_': 18,\n",
       " '[': 19,\n",
       " ']': 20,\n",
       " ',': 21,\n",
       " 'inf': 22,\n",
       " '-': 23,\n",
       " '.': 24,\n",
       " '0': 25,\n",
       " '1': 26,\n",
       " '2': 27,\n",
       " '3': 28,\n",
       " '4': 29,\n",
       " '5': 30,\n",
       " '6': 31,\n",
       " '7': 32,\n",
       " '8': 33,\n",
       " '9': 34}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1009748-3db0-480e-9438-ef1f24acdd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
