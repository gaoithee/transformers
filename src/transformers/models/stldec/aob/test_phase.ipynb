{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe8a622-32bd-426c-8b10-e4ff56e0189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader \n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce4e2977-1cb5-4898-9d84-e13e28c186a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from handcoded_tokenizer import STLTokenizer\n",
    "from configuration import STLConfig\n",
    "from modeling_stldec import STLForCausalLM\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6b99c7-8fac-45b2-ab6d-c44bceb448ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"STLdec\", STLConfig)\n",
    "AutoModelForCausalLM.register(STLConfig, STLForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8985024a-121e-48cd-9987-d98b5f748f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = STLConfig()\n",
    "\n",
    "model_path = \"output_test/epoch_2\"\n",
    "# Carica il modello e spostalo sulla device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config=config).to(device)  # Sposta il modello sulla device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4ed089-eae3-40cc-b511-2b84b33c0802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza l'Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Definisci i percorsi\n",
    "optimizer_path = \"output_test/epoch_2/optimizer.bin\"\n",
    "scheduler_path = \"output_test/epoch_2/scheduler.bin\"\n",
    "\n",
    "# Carica lo stato dell'ottimizzatore e dello scheduler, se necessario\n",
    "# Questi passi dipendono dalla libreria che stai usando, di seguito un esempio generico\n",
    "optimizer = torch.load(optimizer_path)\n",
    "scheduler = torch.load(scheduler_path)\n",
    "\n",
    "optimizer = accelerator.prepare(optimizer)\n",
    "scheduler = accelerator.prepare(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85818cd2-d8e9-40e8-868e-0f9fae9d8806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Formula</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Encoded_Formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11040</td>\n",
       "      <td>11040</td>\n",
       "      <td>eventually[19,21] ( always[7,18] ( x_1 &lt;= 1.17...</td>\n",
       "      <td>tensor([0.06276094168424606, 0.024036630988121...</td>\n",
       "      <td>[2, 1, 7, 19, 26, 34, 21, 27, 26, 20, 1, 4, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31020</td>\n",
       "      <td>31020</td>\n",
       "      <td>( ( ( x_2 &gt;= 0.0125 and x_1 &lt;= -0.4342 ) and (...</td>\n",
       "      <td>tensor([0.000540480890776962, 0.00209438544698...</td>\n",
       "      <td>[2, 1, 4, 1, 4, 1, 4, 1, 17, 18, 27, 1, 12, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33594</td>\n",
       "      <td>33594</td>\n",
       "      <td>( ( ( ( x_2 &gt;= 0.3983 until[15,18] eventually[...</td>\n",
       "      <td>tensor([0.006538981106132269, 0.00377054791897...</td>\n",
       "      <td>[2, 1, 4, 1, 4, 1, 4, 1, 4, 1, 17, 18, 27, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>2401</td>\n",
       "      <td>always[13,18] ( x_2 &gt;= 1.2213 )</td>\n",
       "      <td>tensor([0.0006358523387461901, 0.0017320667393...</td>\n",
       "      <td>[2, 1, 6, 19, 26, 28, 21, 26, 33, 20, 1, 4, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27801</td>\n",
       "      <td>27801</td>\n",
       "      <td>( x_0 &gt;= -0.6846 until[8,inf] always[10,12] ( ...</td>\n",
       "      <td>tensor([0.2162070870399475, 0.7531894445419312...</td>\n",
       "      <td>[2, 1, 4, 1, 17, 18, 25, 1, 12, 1, 23, 25, 24,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0         11040       11040   \n",
       "1         31020       31020   \n",
       "2         33594       33594   \n",
       "3          2401        2401   \n",
       "4         27801       27801   \n",
       "\n",
       "                                             Formula  \\\n",
       "0  eventually[19,21] ( always[7,18] ( x_1 <= 1.17...   \n",
       "1  ( ( ( x_2 >= 0.0125 and x_1 <= -0.4342 ) and (...   \n",
       "2  ( ( ( ( x_2 >= 0.3983 until[15,18] eventually[...   \n",
       "3                    always[13,18] ( x_2 >= 1.2213 )   \n",
       "4  ( x_0 >= -0.6846 until[8,inf] always[10,12] ( ...   \n",
       "\n",
       "                                           Embedding  \\\n",
       "0  tensor([0.06276094168424606, 0.024036630988121...   \n",
       "1  tensor([0.000540480890776962, 0.00209438544698...   \n",
       "2  tensor([0.006538981106132269, 0.00377054791897...   \n",
       "3  tensor([0.0006358523387461901, 0.0017320667393...   \n",
       "4  tensor([0.2162070870399475, 0.7531894445419312...   \n",
       "\n",
       "                                     Encoded_Formula  \n",
       "0  [2, 1, 7, 19, 26, 34, 21, 27, 26, 20, 1, 4, 1,...  \n",
       "1  [2, 1, 4, 1, 4, 1, 4, 1, 17, 18, 27, 1, 12, 1,...  \n",
       "2  [2, 1, 4, 1, 4, 1, 4, 1, 4, 1, 17, 18, 27, 1, ...  \n",
       "3  [2, 1, 6, 19, 26, 28, 21, 26, 33, 20, 1, 4, 1,...  \n",
       "4  [2, 1, 4, 1, 17, 18, 25, 1, 12, 1, 23, 25, 24,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"test_set.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71075d1a-f13d-4c5c-b442-eef5eebedc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, device='cpu'):\n",
    "        self.df = df\n",
    "        self.device = device  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Start from `Encoded_Formula`\n",
    "        encoded_formula = self.df['Encoded_Formula'][idx]\n",
    "        encoded_formula = ast.literal_eval(encoded_formula.strip())\n",
    "        \n",
    "        input_ids = encoded_formula[:-1]  # Tutti tranne l'ultimo\n",
    "        labels = encoded_formula[1:]     # Tutti tranne il primo\n",
    "\n",
    "        attention_mask = [0 if token == '1' else 1 for token in input_ids]\n",
    "        # if 1 (i.e. tokenized `pad`), then neglect that token\n",
    "\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).to(self.device)\n",
    "        labels = torch.tensor(labels, dtype=torch.long).to(self.device)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(self.device)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'attention_mask': attention_mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27fed010-da2f-4c72-8c3f-b86e025b3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_df, device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f00dfad-f104-40e5-b1ff-4b6613aed49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "\n",
    "model.eval()\n",
    "losses = []\n",
    "for step, batch in enumerate(test_loader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)   \n",
    "    loss = outputs.loss\n",
    "    losses.append(accelerator.gather_for_metrics(loss.repeat(16)))\n",
    "\n",
    "losses = torch.cat(losses)\n",
    "try:\n",
    "    eval_loss = torch.mean(losses)\n",
    "    perplexity = math.exp(eval_loss)\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6ae932b-2692-4695-8342-665f189a28af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.45194780826568604\n",
      "Perplexity: 1.571369933915324\n"
     ]
    }
   ],
   "source": [
    "# Visualizza le metriche\n",
    "accelerator.print(f\"Eval Loss: {eval_loss.item()}\")  # Visualizza la perdita\n",
    "accelerator.print(f\"Perplexity: {perplexity}\")      # Visualizza la perplessità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c63817ca-4fef-422f-842e-1616a4b60eb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (STLForCausalLM) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'STLForCausalLM'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Genera la sequenza autoregressiva\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 21\u001b[0m     generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Usa gli ID tokenizzati\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ID del token di padding, se presente\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Decodifica e visualizza il testo generato\u001b[39;00m\n\u001b[1;32m     27\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(generated_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/utils.py:1674\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m \n\u001b[1;32m   1592\u001b[0m \u001b[38;5;124;03mGenerates sequences of token ids for models with a language modeling head.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;124;03m            - [`~generation.GenerateBeamEncoderDecoderOutput`]\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 1674\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1675\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/orfeo/cephfs/home/dssc/scandu00/nlp-env/lib64/python3.9/site-packages/transformers/generation/utils.py:1172\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate_compatible_classes:\n\u001b[1;32m   1171\u001b[0m     exception_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please use one of the following classes instead: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerate_compatible_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(exception_message)\n",
      "\u001b[0;31mTypeError\u001b[0m: The current model class (STLForCausalLM) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'STLForCausalLM'}"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Supponiamo che `test_df` contenga già la colonna `Encoded_Formula` con i token\n",
    "\n",
    "# Estrai un esempio a caso dal test_df\n",
    "example_idx = 0  # Puoi scegliere un altro indice o usare random per un campione casuale\n",
    "encoded_formula = ast.literal_eval(test_df['Encoded_Formula'][example_idx].strip())  # Decodifica la lista di token\n",
    "\n",
    "# Converti il vettore di token in un tensor\n",
    "input_ids = torch.tensor(encoded_formula, dtype=torch.long).unsqueeze(0).to(model.device)  # Aggiungi la dimensione batch\n",
    "\n",
    "# Impostazioni per la generazione autoregressiva\n",
    "max_length = 50  # Lunghezza massima della sequenza generata\n",
    "temperature = 1.0  # Controlla la casualità (1.0 = più casuale, 0.0 = deterministico)\n",
    "top_k = 50  # Top-k sampling\n",
    "top_p = 0.95  # Top-p sampling (nucleus sampling)\n",
    "num_return_sequences = 1  # Numero di sequenze da generare\n",
    "\n",
    "# Genera la sequenza autoregressiva\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,  # Usa gli ID tokenizzati\n",
    "        pad_token_id=model.config.pad_token_id,  # ID del token di padding, se presente\n",
    "    )\n",
    "\n",
    "# Decodifica e visualizza il testo generato\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47da50d4-fba9-44ed-ad25-a574adf85849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STLForCausalLM(\n",
       "  (model): STLDecoder(\n",
       "    (embed_tokens): Embedding(35, 1024, padding_idx=1)\n",
       "    (embed_positions): STLSinusoidalPositionalEmbedding(1024, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x STLDecoderBlock(\n",
       "        (self_attn): STLAttention(\n",
       "          (W_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): STLAttention(\n",
       "          (W_k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=35, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ef2c6-7111-486a-a384-5f29f3c6b8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
