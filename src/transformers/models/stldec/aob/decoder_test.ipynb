{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67a7608-45d8-4ac1-bf7d-e8c07b51d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import STLEncoder\n",
    "from handcoded_tokenizer import STLTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ba17d9-28e5-42dd-90ea-bd3a0c495b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "formulae_to_embed = [\n",
    "    'not ( x_1 <= 0.0956 )', \n",
    "    'not ( x_2 >= 1.118 )', \n",
    "    'not ( ( not ( x_0 <= -0.692 ) and ( eventually[8,19] ( x_2 <= -1.5116 ) until[6,inf] x_2 >= -0.3382 ) ) )', \n",
    "    '( ( x_2 >= -0.4612 or x_1 <= -1.1656 ) or x_0 <= -0.8679 )']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293de6a6-5cf6-4f7b-8a2d-1994f3d81194",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = STLEncoder(embed_dim=10, anchor_filename='anchor_set_10_dim.pickle')\n",
    "formulae_embeddings = encoder.compute_embeddings(formulae_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9279dcd-9d8a-41ca-97f6-564ef86221a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 11, 1, 4, 1, 17, 18, 26, 1, 12, 1, 25, 24, 25, 34, 30, 31, 1, 5, 1, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'not ( x_1 >= 0.0956 )'\n",
    "tokenizer = STLTokenizer('tokenizer_files/tokenizer.json')\n",
    "tokenizer.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d669f07f-f687-4920-8a42-ccc60455c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6146a88f-ab55-462c-a4f0-c4a79a4a3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils2 import STLAttention, STLSinusoidalPositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4026494-c615-4f73-944d-31dd061a2bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->Marian, BART->MARIAN\n",
    "class STLDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim: int, \n",
    "                num_decoder_attention_heads: int,\n",
    "                num_decoder_ffn_dim: int,\n",
    "                dropout: float = 0.0,\n",
    "                attention_dropout: float = 0.0,\n",
    "                activation_dropout: float = 0.0,\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # first block\n",
    "        self.self_attn = STLAttention(\n",
    "            embed_dim=self.embed_dim, \n",
    "            num_heads=num_decoder_attention_heads,\n",
    "            dropout=dropout,\n",
    "            is_decoder=True, # not used\n",
    "            is_causal=True, # not used\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "        self.activation_fn = nn.functional.gelu\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # second block\n",
    "        self.encoder_attn = STLAttention(\n",
    "            self.embed_dim,\n",
    "            num_decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            is_decoder=True, # not used\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "        # third block\n",
    "        self.fc1 = nn.Linear(self.embed_dim, num_decoder_ffn_dim)\n",
    "        self.fc2 = nn.Linear(num_decoder_ffn_dim, self.embed_dim)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_layer_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = True,\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            attention_mask (`torch.FloatTensor`): attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            encoder_hidden_states (`torch.FloatTensor`):\n",
    "                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n",
    "            encoder_attention_mask (`torch.FloatTensor`): encoder attention mask of size\n",
    "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
    "            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
    "                `(encoder_attention_heads,)`.\n",
    "            cross_attn_layer_head_mask (`torch.FloatTensor`): mask for cross-attention heads in a given layer of\n",
    "                size `(decoder_attention_heads,)`.\n",
    "            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "        \"\"\"\n",
    "        \n",
    "        ###################################################################\n",
    "        \n",
    "        # BLOCK 1: processing what has been previously generated \n",
    "\n",
    "        # previous state is stored into an auxiliary variable `residual`\n",
    "        residual = hidden_states\n",
    "\n",
    "        # tries to exploit previous K, V values if there are any \n",
    "        # (practically picks up to the first 2 values stored in `past_key_value` vector)\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "\n",
    "        # masked MHSA on the already generated sequence\n",
    "        # invokes `forward` method to transform the original vector accordingly \n",
    "        hidden_states, self_attn_weights, present_key_value = self.self_attn.forward(\n",
    "            hidden_states=hidden_states, # Q\n",
    "            past_key_value=self_attn_past_key_value, # K, V\n",
    "            attention_mask=attention_mask, # passed as input of the decoder layer\n",
    "            layer_head_mask=layer_head_mask, # to deactivate certain attn layers \n",
    "            output_attentions=output_attentions, \n",
    "        )\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # normalization\n",
    "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
    "\n",
    "        ###################################################################\n",
    "\n",
    "        # BLOCK 2: cross-attn between already generated input and previous information (from the encoder)\n",
    "\n",
    "        # initialize K, Q, attn_weights for this new attn operation\n",
    "        cross_attn_present_key_value = None \n",
    "        cross_attn_weights = None\n",
    "\n",
    "        # the important condition is that the encoder carries some information\n",
    "        if encoder_hidden_states is not None:\n",
    "\n",
    "            # previous state is stored into an auxiliary variable `residual`\n",
    "            residual = hidden_states\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3, 4 of PAST_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "\n",
    "            # MHSA in cross-attn\n",
    "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn.forward(\n",
    "                hidden_states=hidden_states, # Q = generated output\n",
    "                key_value_states=encoder_hidden_states, # K, V = encoder memory (used only in the 1st step when `use_cache = True`)\n",
    "                layer_head_mask=cross_attn_layer_head_mask, # again to mask certain heads\n",
    "                past_key_value=cross_attn_past_key_value, # K, V = encoder CACHED memory (used from the 2nd step on when `use_cache = True`)\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "            # residual connection\n",
    "            hidden_states = residual + hidden_states\n",
    "\n",
    "            # normalization\n",
    "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
    "\n",
    "            # add cross-attn to positions 3, 4 of PRESENT_key_value tuple\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        ###################################################################\n",
    "\n",
    "        # BLOCK 3: FFNN (transforming some merged generated output - encoder information)\n",
    "\n",
    "        # previous state is stored into an auxiliary variable `residual`\n",
    "        residual = hidden_states\n",
    "\n",
    "        # FFNN - core\n",
    "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # residual connection\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # normalization\n",
    "        hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights, cross_attn_weights)\n",
    "\n",
    "        if use_cache: # if not, picks again K and V each time\n",
    "            outputs += (present_key_value,)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41edd767-a8e2-4a21-bb71-9b199b1b265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from configuration import STLConfig\n",
    "\n",
    "class STLPreTrainedModel(PreTrainedModel):\n",
    "    config_class = STLConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "\n",
    "    def _init_weights(self, module: Union[nn.Linear, nn.Embedding, STLSinusoidalPositionalEmbedding]):\n",
    "        std = self.config.init_std\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, MarianSinusoidalPositionalEmbedding):\n",
    "            pass\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    @property\n",
    "    def dummy_inputs(self):\n",
    "        pad_token = self.config.pad_token_id\n",
    "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
    "        dummy_inputs = {\n",
    "            \"attention_mask\": input_ids.ne(pad_token),\n",
    "            \"input_ids\": input_ids,\n",
    "            \"decoder_input_ids\": input_ids,\n",
    "        }\n",
    "        return dummy_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84038673-9cc9-4530-bbcd-5288e011291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    ")\n",
    "\n",
    "class STLDecoder(STLPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *num_decoder_layers* layers. Each layer is a [`STLDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        configs\n",
    "        embed_tokens (nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, embed_dim: int, \n",
    "                num_decoder_attention_heads: int,\n",
    "                num_decoder_ffn_dim: int,\n",
    "                max_position_embeddings: int,\n",
    "                decoder_vocab_size: int,\n",
    "                pad_token_id: str,\n",
    "                num_decoder_layers: int = 8,\n",
    "                scale_embedding: bool = False,\n",
    "                dropout: float = 0.0,\n",
    "                attention_dropout: float = 0.0,\n",
    "                activation_dropout: float = 0.0,\n",
    "                decoder_layerdrop: float = 0.0,\n",
    "                \n",
    "                # encoder output:\n",
    "                embed_tokens: Optional[nn.Embedding] = None,\n",
    "                ):\n",
    "        \n",
    "        super().__init__(config)\n",
    "        self.dropout = dropout\n",
    "        self.layerdrop = decoder_layerdrop\n",
    "        self.padding_idx = pad_token_id\n",
    "        self.max_target_positions = max_position_embeddings\n",
    "        self.embed_scale = math.sqrt(embed_dim) if scale_embedding else 1.0\n",
    "\n",
    "        if embed_tokens is not None:\n",
    "            self.embed_tokens = embed_tokens\n",
    "        else:\n",
    "            self.embed_tokens = nn.Embedding(decoder_vocab_size, embed_dim, self.padding_idx)\n",
    "\n",
    "        self.embed_positions = MarianSinusoidalPositionalEmbedding(\n",
    "            max_position_embeddings, embed_dim, self.padding_idx\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.ModuleList([STLDecoderLayer(embed_dim, num_decoder_attention_heads,\n",
    "                                                        num_decoder_ffn_dim, dropout, \n",
    "                                                        attention_dropout, activation_dropout) for _ in range(num_decoder_layers)])\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    "\n",
    "        attention_mask = _prepare_4d_causal_attention_mask(\n",
    "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
    "        )\n",
    "\n",
    "        # expand encoder attention mask\n",
    "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            encoder_attention_mask = _prepare_4d_attention_mask(\n",
    "                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n",
    "            )\n",
    "\n",
    "        # embed positions\n",
    "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
    "\n",
    "        hidden_states = inputs_embeds + positions\n",
    "\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        if self.gradient_checkpointing and self.training:\n",
    "            if use_cache:\n",
    "                logger.warning_once(\n",
    "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "\n",
    "        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired\n",
    "        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], [\"head_mask\", \"cross_attn_head_mask\"]):\n",
    "            if attn_mask is not None:\n",
    "                assert attn_mask.size()[0] == (len(self.layers)), (\n",
    "                    f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "            if self.training:\n",
    "                dropout_probability = torch.rand([])\n",
    "                if dropout_probability < self.layerdrop:\n",
    "                    continue\n",
    "\n",
    "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    head_mask[idx] if head_mask is not None else None,\n",
    "                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n",
    "                    None,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                    cross_attn_layer_head_mask=(\n",
    "                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n",
    "                    ),\n",
    "                    past_key_value=past_key_value,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "                if encoder_hidden_states is not None:\n",
    "                    all_cross_attentions += (layer_outputs[2],)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        next_cache = next_decoder_cache if use_cache else None\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82334eac-9c42-430a-84d7-40a7f21154a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
