{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee18e21b-8ca4-4024-96ca-d8c2c3ec850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from phis_generator import StlGenerator\n",
    "from traj_measure import BaseMeasure\n",
    "from utils import from_string_to_formula, load_pickle, dump_pickle\n",
    "from kernel import StlKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbca254-a717-4ba0-b4a4-eb95ba77dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anchorGeneration(diff_init = False, # to control whether we want formulae to be semantically different by construction\n",
    "                     embed_dim: int = 30, # embedding dimension, aka number of generated formulae in the anchor set\n",
    "                     n_vars: int = 3, # dimension of the input signal (3D in this case)\n",
    "                     leaf_prob: float = 0.4, # complexity of the generated formula\n",
    "                     cosine_similarity_threshold: float = 0.8 # if two formulae cosine similarity exceeds 0.9, then discard one of the two\n",
    "                    ):\n",
    "    \n",
    "    # initialize STL formula generator\n",
    "    sampler = StlGenerator(leaf_prob)\n",
    "    \n",
    "    # effective anchor set generation\n",
    "    if diff_init:\n",
    "        \n",
    "        # initialize the anchor set with a randomly sampled formula\n",
    "        diff_anchor_set = [sampler.sample(nvars=n_vars)]\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mu = BaseMeasure(device=device)\n",
    "\n",
    "        # generates a set of random signals working as a tester for the formulae testing\n",
    "        signals = mu.sample(samples=10000, varn=n_vars)\n",
    "\n",
    "        # computes robustness value for the initial set of formulae in the anchor set\n",
    "        anchor_rob_vectors = torch.cat([phi.quantitative(signals, normalize=True).unsqueeze(0) for phi in diff_anchor_set], 0)\n",
    "\n",
    "        while len(diff_anchor_set) < embed_dim:\n",
    "            # sample the 'remaining' formulae to reach the desired number of `embed_dim` formulae:\n",
    "            candidate_anchors = sampler.bag_sample(embed_dim - len(diff_anchor_set), nvars = n_vars)\n",
    "    \n",
    "            # compute robustness of candidate anchor formulae on the same signals as previous anchor set\n",
    "            candidate_robs = torch.cat([phi.quantitative(signals, normalize=True).unsqueeze(0) for phi in candidate_anchors], 0)\n",
    "            \n",
    "            # compute cosine similarity between current anchor set and candidate new formulae\n",
    "            cos_simil = torch.tril(normalize(candidate_robs) @ normalize(anchor_rob_vectors).t(), diagonal=-1)\n",
    "\n",
    "            # check which formulae are similar (i.e. greater cosine similarity then threshold) w.r.t. current anchors\n",
    "            # NOTA: chiedere a gaia se cosine similarities negative vanno ammazzate con un valore assoluto o meno!\n",
    "            similar_idx = [torch.where(cos_simil[r, :] > cosine_similarity_threshold)[0].tolist() for r in range(cos_simil.shape[0])]\n",
    "    \n",
    "            # keep only those who are semantically distant\n",
    "            keep_idx = list(set(np.arange(len(candidate_anchors)).tolist()).difference(set([i for sublist in similar_idx for i in sublist])))\n",
    "            \n",
    "            diff_anchor_set += [copy.deepcopy(candidate_anchors[i]) for i in keep_idx]\n",
    "            \n",
    "            # Convert keep_idx to a tensor on the same device as candidate_robs\n",
    "            keep_idx_tensor = torch.tensor(keep_idx, device=candidate_robs.device)\n",
    "            \n",
    "            # Use index_select to pick the relevant rows\n",
    "            selected_robs = torch.index_select(candidate_robs, 0, keep_idx_tensor)\n",
    "            \n",
    "            # Concatenate on the same device\n",
    "            anchor_rob_vectors = torch.cat([anchor_rob_vectors, copy.deepcopy(selected_robs)], dim=0)\n",
    "\n",
    "            anchor_set = diff_anchor_set[:embed_dim]\n",
    "            \n",
    "    else:\n",
    "        anchor_set = sampler.bag_sample(bag_size=embed_dim, nvars=n_vars) \n",
    "\n",
    "    filename = 'anchor_set_{}_dim'.format(embed_dim)\n",
    "    dump_pickle(filename, anchor_set)\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d68236-45c6-4343-866e-d8803ad7d27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_set_20_dim\n"
     ]
    }
   ],
   "source": [
    "anchorGeneration(diff_init = True, embed_dim = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d2d85-de6c-4858-b345-adbcb0057f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
