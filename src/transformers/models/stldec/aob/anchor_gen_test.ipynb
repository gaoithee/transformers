{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5d9609-8970-4e16-b308-827e8afec28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2d9b95-b630-494f-91b8-fe9193a8b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_utils import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06562782-9de0-4b99-9502-1069dbe79d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StlKernel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        measure,\n",
    "        normalize=True,\n",
    "        exp_kernel=True,\n",
    "        sigma2=0.2,\n",
    "        integrate_time=False,\n",
    "        samples=100000,\n",
    "        varn=2,\n",
    "        points=100,\n",
    "        boolean=False,\n",
    "        signals=None,\n",
    "    ):\n",
    "        self.traj_measure = measure\n",
    "        self.exp_kernel = exp_kernel\n",
    "        self.normalize = normalize\n",
    "        self.sigma2 = sigma2\n",
    "        self.samples = samples\n",
    "        self.varn = varn\n",
    "        self.points = points\n",
    "        self.integrate_time = integrate_time\n",
    "        if signals is not None:\n",
    "            self.signals = signals\n",
    "        else:\n",
    "            self.signals = measure.sample(points=points, samples=samples, varn=varn)\n",
    "        self.boolean = boolean\n",
    "\n",
    "    def compute(self, phi1, phi2):\n",
    "        return self.compute_one_one(phi1, phi2)\n",
    "\n",
    "    def compute_one_one(self, phi1, phi2):\n",
    "        phis1: list = [phi1]\n",
    "        phis2: list = [phi2]\n",
    "        ker = self.compute_bag_bag(phis1, phis2)\n",
    "        return ker[0, 0]\n",
    "\n",
    "    def compute_bag(self, phis, return_robustness=True):\n",
    "        if self.integrate_time:\n",
    "            rhos, selfk, len0 = self._compute_robustness_time(phis)\n",
    "            kernel_matrix = self._compute_kernel_time(\n",
    "                rhos, rhos, selfk, selfk, len0, len0\n",
    "            )\n",
    "        else:\n",
    "            rhos, selfk = self._compute_robustness_no_time(phis)\n",
    "            kernel_matrix = self._compute_kernel_no_time(rhos, rhos, selfk, selfk)\n",
    "            len0 = None\n",
    "        if return_robustness:\n",
    "            return kernel_matrix.cpu(), rhos, selfk, len0\n",
    "        else:\n",
    "            return kernel_matrix.cpu()\n",
    "\n",
    "    def compute_one_bag(self, phi1, phis2, return_robustness=False):\n",
    "        phis1: list = [phi1]\n",
    "        return self.compute_bag_bag(phis1, phis2, return_robustness)\n",
    "\n",
    "    def compute_bag_bag(self, phis1, phis2, return_robustness=False):\n",
    "        if self.integrate_time:\n",
    "            rhos1, selfk1, len1 = self._compute_robustness_time(phis1)\n",
    "            rhos2, selfk2, len2 = self._compute_robustness_time(phis2)\n",
    "            kernel_matrix = self._compute_kernel_time(\n",
    "                rhos1, rhos2, selfk1, selfk2, len1, len2\n",
    "            )\n",
    "        else:\n",
    "            rhos1, selfk1 = self._compute_robustness_no_time(phis1)\n",
    "            rhos2, selfk2 = self._compute_robustness_no_time(phis2)\n",
    "            len1, len2 = [None, None]\n",
    "            kernel_matrix = self._compute_kernel_no_time(rhos1, rhos2, selfk1, selfk2)\n",
    "        if return_robustness:\n",
    "            return kernel_matrix.cpu(), rhos1, rhos2, selfk1, selfk2, len1, len2\n",
    "        else:\n",
    "            return kernel_matrix.cpu()\n",
    "\n",
    "    def compute_one_from_robustness(self, phi, rhos, rho_self, lengths=None, return_robustness=False):\n",
    "        phis: list = [phi]\n",
    "        return self.compute_bag_from_robustness(phis, rhos, rho_self, lengths, return_robustness)\n",
    "\n",
    "    def compute_bag_from_robustness(self, phis, rhos, rho_self, lengths=None, return_robustness=False):\n",
    "        if self.integrate_time:\n",
    "            rhos1, selfk1, len1 = self._compute_robustness_time(phis)\n",
    "            kernel_matrix = self._compute_kernel_time(\n",
    "                rhos1, rhos, selfk1, rho_self, len1, lengths\n",
    "            )\n",
    "        else:\n",
    "            rhos1, selfk1 = self._compute_robustness_no_time(phis)\n",
    "            len1 = None\n",
    "            kernel_matrix = self._compute_kernel_no_time(rhos1, rhos, selfk1, rho_self)\n",
    "        if return_robustness:\n",
    "            return kernel_matrix.cpu(), rhos1, selfk1, len1\n",
    "        else:\n",
    "            return kernel_matrix.cpu()\n",
    "\n",
    "    def _compute_robustness_time(self, phis):\n",
    "        n = self.samples\n",
    "        p = self.points\n",
    "        k = len(phis)\n",
    "        rhos = torch.zeros((k, n, p), device=\"cpu\")\n",
    "        lengths = torch.zeros(k)\n",
    "        self_kernels = torch.zeros((k, 1))\n",
    "        for i, phi in enumerate(phis):\n",
    "            if self.boolean:\n",
    "                rho = phi.boolean(self.signals, evaluate_at_all_times=True).float()\n",
    "                rho[rho == 0.0] = -1.0\n",
    "            else:\n",
    "                rho = phi.quantitative(self.signals, evaluate_at_all_times=True)\n",
    "            actual_p = rho.size()[2]\n",
    "            rho = rho.reshape(n, actual_p).cpu()\n",
    "            rhos[i, :, :actual_p] = rho\n",
    "            lengths[i] = actual_p\n",
    "            self_kernels[i] = torch.tensordot(\n",
    "                rho.reshape(1, n, -1), rho.reshape(1, n, -1), dims=[[1, 2], [1, 2]]\n",
    "            ) / (actual_p * n)\n",
    "        return rhos, self_kernels, lengths\n",
    "\n",
    "    def _compute_robustness_no_time(self, phis):\n",
    "        n = self.samples\n",
    "        k = len(phis)\n",
    "        rhos = torch.zeros((k, n), device=self.traj_measure.device)\n",
    "        self_kernels = torch.zeros((k, 1), device=self.traj_measure.device)\n",
    "        for i, phi in enumerate(phis):\n",
    "            if self.boolean:\n",
    "                rho = phi.boolean(self.signals, evaluate_at_all_times=False).float()\n",
    "                rho[rho == 0.0] = -1.0\n",
    "            else:\n",
    "                rho = phi.quantitative(self.signals, evaluate_at_all_times=False)\n",
    "            self_kernels[i] = rho.dot(rho) / n\n",
    "            rhos[i, :] = rho\n",
    "        return rhos, self_kernels\n",
    "\n",
    "    def _compute_kernel_time(self, rhos1, rhos2, selfk1, selfk2, len1, len2):\n",
    "        kernel_matrix = torch.tensordot(rhos1, rhos2, [[1, 2], [1, 2]])\n",
    "        length_normalizer = self._compute_trajectory_length_normalizer(len1, len2)\n",
    "        kernel_matrix = kernel_matrix * length_normalizer / self.samples\n",
    "        if self.normalize:\n",
    "            kernel_matrix = self._normalize(kernel_matrix, selfk1, selfk2)\n",
    "        if self.exp_kernel:\n",
    "            kernel_matrix = self._exponentiate(kernel_matrix, selfk1, selfk2)\n",
    "        return kernel_matrix\n",
    "\n",
    "    def _compute_kernel_no_time(self, rhos1, rhos2, selfk1, selfk2):\n",
    "        kernel_matrix = torch.tensordot(rhos1, rhos2, [[1], [1]])\n",
    "        kernel_matrix = kernel_matrix / self.samples\n",
    "        if self.normalize:\n",
    "            kernel_matrix = self._normalize(kernel_matrix, selfk1, selfk2)\n",
    "        if self.exp_kernel:\n",
    "            kernel_matrix = self._exponentiate(kernel_matrix, selfk1, selfk2)\n",
    "        return kernel_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize(kernel_matrix, selfk1, selfk2):\n",
    "        normalize = torch.sqrt(torch.matmul(selfk1, torch.transpose(selfk2, 0, 1)))\n",
    "        kernel_matrix = kernel_matrix / normalize\n",
    "        return kernel_matrix\n",
    "\n",
    "    def _exponentiate(self, kernel_matrix, selfk1, selfk2, sigma2=None):\n",
    "        if sigma2 is None:\n",
    "            sigma2 = self.sigma2\n",
    "        if self.normalize:\n",
    "            # selfk is (1.0^2 + 1.0^2)\n",
    "            selfk = 2.0\n",
    "        else:\n",
    "            k1 = selfk1.size()[0]\n",
    "            k2 = selfk2.size()[0]\n",
    "            selfk = (selfk1 * selfk1).repeat(1, k2) + torch.transpose(\n",
    "                selfk2 * selfk2, 0, 1\n",
    "            ).repeat(k1, 1)\n",
    "        return torch.exp(-(selfk - 2 * kernel_matrix) / (2 * sigma2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_trajectory_length_normalizer(len1, len2):\n",
    "        k1 = len1.size()[0]\n",
    "        k2 = len2.size()[0]\n",
    "        y1 = len1.reshape(-1, 1)\n",
    "        y1 = y1.repeat(1, k2)\n",
    "        y2 = len2.repeat(k1, 1)\n",
    "        return 1.0 / torch.min(y1, y2)\n",
    "\n",
    "\n",
    "class GramMatrix:\n",
    "    def __init__(self, kernel, formulae, store_robustness=True, sample=False, sampler=None, bag_size=None):\n",
    "        self.kernel = kernel\n",
    "        self.formulae_list = formulae\n",
    "        # if kernel is computed from robustness at time zero only,\n",
    "        # we store the robustness for each formula and each sample\n",
    "        # to speed up computation later\n",
    "        self.store_robustness = store_robustness\n",
    "        self.dim = len(self.formulae_list) if not bag_size else int(bag_size)\n",
    "        self.sample = sample  # whether to generate formulae in a controlled manner\n",
    "        if self.sample:\n",
    "            self.t = 0.99 if self.kernel.boolean else 0.85\n",
    "        self.sampler = sampler  # stl formulae generator\n",
    "        self._compute_gram_matrix()\n",
    "\n",
    "    def _compute_gram_matrix(self):\n",
    "        if self.sample:\n",
    "            gram = torch.zeros(self.dim, self.dim)\n",
    "            rhos = torch.zeros((self.dim, self.kernel.samples), device=self.kernel.traj_measure.device) if \\\n",
    "                not self.kernel.integrate_time else torch.zeros((self.dim, self.kernel.samples, self.kernel.points),\n",
    "                                                                device=self.kernel.traj_measure.device)\n",
    "            lengths = torch.zeros(self.dim) if self.kernel.integrate_time else np.zeros(self.dim)\n",
    "            kernels = torch.zeros((self.dim, 1), device=self.kernel.traj_measure.device)\n",
    "            phis = [self.sampler.sample(nvars=self.kernel.varn)]\n",
    "            gram[0, :1], rhos[0], kernels[0, :], lengths[0] = self.kernel.compute_bag(phis, return_robustness=True)\n",
    "            while len(phis) < self.dim:\n",
    "                i = len(phis)\n",
    "                phi = self.sampler.sample(nvars=self.kernel.varn)\n",
    "                gram[i, :i], rhos[i], kernels[i, :], lengths[i] = self.kernel.compute_one_from_robustness(\n",
    "                    phi, rhos[:i, :], kernels[:i, :], lengths[:i], return_robustness=True)\n",
    "                if torch.sum(gram[i, :i + 1] >= self.t) < 3:\n",
    "                    phis.append(phi)\n",
    "                    gram[:i, i] = gram[i, :i]\n",
    "                    gram[i, i] = kernels[i, :]\n",
    "\n",
    "            self.formulae_list = phis\n",
    "            self.gram = gram.cpu()\n",
    "            self.robustness = rhos if self.store_robustness else None\n",
    "            self.self_kernels = kernels if self.store_robustness else None\n",
    "            self.robustness_lengths = lengths if self.store_robustness else None\n",
    "        else:\n",
    "            if self.store_robustness:\n",
    "                k_matrix, rhos, selfk, len0 = self.kernel.compute_bag(\n",
    "                    self.formulae_list, return_robustness=True\n",
    "                )\n",
    "                self.gram = k_matrix\n",
    "                self.robustness = rhos\n",
    "                self.self_kernels = selfk\n",
    "                self.robustness_lengths = len0\n",
    "            else:\n",
    "                self.gram = self.kernel.compute_bag(\n",
    "                    self.formulae_list, return_robustness=False\n",
    "                )\n",
    "                self.robustness = None\n",
    "                self.self_kernels = None\n",
    "                self.robustness_lengths = None\n",
    "\n",
    "    def compute_kernel_vector(self, phi):\n",
    "        if self.store_robustness:\n",
    "            return self.kernel.compute_one_from_robustness(\n",
    "                phi, self.robustness, self.self_kernels, self.robustness_lengths\n",
    "            )\n",
    "        else:\n",
    "            return self.kernel.compute_one_bag(phi, self.formulae_list)\n",
    "\n",
    "    def compute_bag_kernel_vector(self, phis, generate_phis=False, bag_size=None):\n",
    "        if generate_phis:\n",
    "            gram_test = torch.zeros(bag_size, self.dim)  # self.dim, bag_size\n",
    "            rhos_test = torch.zeros((bag_size, self.kernel.samples), device=self.kernel.traj_measure.device) if \\\n",
    "                not self.kernel.integrate_time else torch.zeros((bag_size, self.kernel.samples, self.kernel.points),\n",
    "                                                                device=self.kernel.traj_measure.device)\n",
    "            lengths_test = torch.zeros(bag_size) if self.kernel.integrate_time else np.zeros(bag_size)\n",
    "            kernels_test = torch.zeros((bag_size, 1), device=self.kernel.traj_measure.device)\n",
    "            phi_test = []\n",
    "            while len(phi_test) < bag_size:\n",
    "                i = len(phi_test)\n",
    "                phi = self.sampler.sample(nvars=self.kernel.varn)\n",
    "                if self.store_robustness:\n",
    "                    gram_test[i, :], rhos_test[i], kernels_test[i, :], lengths_test[i] = \\\n",
    "                        self.kernel.compute_one_from_robustness(phi, self.robustness, self.self_kernels,\n",
    "                                                                self.robustness_lengths, return_robustness=True)\n",
    "                else:\n",
    "                    gram_test[i, :], rhos_test[i], _, kernels_test[i, :], _, lengths_test[i], _ = \\\n",
    "                        self.kernel.compute_one_bag(phi, self.formulae_list, return_robustness=True)\n",
    "                if not ((rhos_test[i] > 0).all() or (rhos_test[i] < 0).all()):\n",
    "                    phi_test.append(phi)\n",
    "            return phi_test, gram_test.cpu()\n",
    "        else:\n",
    "            if self.store_robustness:\n",
    "                return self.kernel.compute_bag_from_robustness(\n",
    "                    phis, self.robustness, self.self_kernels, self.robustness_lengths\n",
    "                )\n",
    "            else:\n",
    "                return self.kernel.compute_bag_bag(phis, self.formulae_list)\n",
    "\n",
    "    def invert_regularized(self, alpha):\n",
    "        regularizer = abs(pow(10, alpha)) * torch.eye(self.dim)\n",
    "        return torch.inverse(self.gram + regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9fde05-002c-4923-b019-a28a9be92ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class StlKernel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        measure,\n",
    "        normalize=True,\n",
    "        exp_kernel=True,\n",
    "        sigma2=0.2,\n",
    "        integrate_time=False,\n",
    "        samples=100000,\n",
    "        varn=2,\n",
    "        points=100,\n",
    "        boolean=False,\n",
    "        signals=None,\n",
    "    ):\n",
    "        self.traj_measure = measure\n",
    "        self.exp_kernel = exp_kernel\n",
    "        self.normalize = normalize\n",
    "        self.sigma2 = sigma2\n",
    "        self.samples = samples\n",
    "        self.varn = varn\n",
    "        self.points = points\n",
    "        self.integrate_time = integrate_time\n",
    "        if signals is not None:\n",
    "            self.signals = signals\n",
    "        else:\n",
    "            self.signals = measure.sample(points=points, samples=samples, varn=varn)\n",
    "        self.boolean = boolean\n",
    "\n",
    "    def compute(self, phi1, phi2):\n",
    "        return self.compute_one_one(phi1, phi2)\n",
    "\n",
    "    def compute_one_one(self, phi1, phi2):\n",
    "        phis1: list = [phi1]\n",
    "        phis2: list = [phi2]\n",
    "        ker = self.compute_bag_bag(phis1, phis2)\n",
    "        return ker[0, 0]\n",
    "\n",
    "    def compute_bag(self, phis, return_robustness=True):\n",
    "        if self.integrate_time:\n",
    "            rhos, selfk, len0 = self._compute_robustness_time(phis)\n",
    "            kernel_matrix = self._compute_kernel_time(\n",
    "                rhos, rhos, selfk, selfk, len0, len0\n",
    "            )\n",
    "        else:\n",
    "            rhos, selfk = self._compute_robustness_no_time(phis)\n",
    "            kernel_matrix = self._compute_kernel_no_time(rhos, rhos, selfk, selfk)\n",
    "            len0 = None\n",
    "        if return_robustness:\n",
    "            return kernel_matrix.cpu(), rhos, selfk, len0\n",
    "        else:\n",
    "            return kernel_matrix.cpu()\n",
    "\n",
    "    def compute_one_bag(self, phi1, phis2, return_robustness=False):\n",
    "        phis1: list = [phi1]\n",
    "        return self.compute_bag_bag(phis1, phis2, return_robustness)\n",
    "\n",
    "    def compute_bag_bag(self, phis1, phis2, return_robustness=False):\n",
    "        if self.integrate_time:\n",
    "            rhos1, selfk1, len1 = self._compute_robustness_time(phis1)\n",
    "            rhos2, selfk2, len2 = self._compute_robustness_time(phis2)\n",
    "            kernel_matrix = self._compute_kernel_time(\n",
    "                rhos1, rhos2, selfk1, selfk2, len1, len2\n",
    "            )\n",
    "        else:\n",
    "            rhos1, selfk1 = self._compute_robustness_no_time(phis1)\n",
    "            rhos2, selfk2 = self._compute_robustness_no_time(phis2)\n",
    "            len1, len2 = [None, None]\n",
    "            kernel_matrix = self._compute_kernel_no_time(rhos1, rhos2, selfk1, selfk2)\n",
    "        if return_robustness:\n",
    "            return kernel_matrix.cpu(), rhos1, rhos2, selfk1, selfk2, len1, len2\n",
    "        else:\n",
    "            return kernel_matrix.cpu()\n",
    "\n",
    "    def compute_one_from_robustness(self, phi, rhos, rho_self, lengths=None, return_robustness=False):\n",
    "        phis: list = [phi]\n",
    "        return self.compute_bag_from_robustness(phis, rhos, rho_self, lengths, return_robustness)\n",
    "\n",
    "    def compute_bag_from_robustness(self, phis, rhos, rho_self, lengths=None, return_robustness=False):\n",
    "        if self.integrate_time:\n",
    "            rhos1, selfk1, len1 = self._compute_robustness_time(phis)\n",
    "            kernel_matrix = self._compute_kernel_time(\n",
    "                rhos1, rhos, selfk1, rho_self, len1, lengths\n",
    "            )\n",
    "        else:\n",
    "            rhos1, selfk1 = self._compute_robustness_no_time(phis)\n",
    "            len1 = None\n",
    "            kernel_matrix = self._compute_kernel_no_time(rhos1, rhos, selfk1, rho_self)\n",
    "        if return_robustness:\n",
    "            return kernel_matrix.cpu(), rhos1, selfk1, len1\n",
    "        else:\n",
    "            return kernel_matrix.cpu()\n",
    "\n",
    "    def _compute_robustness_time(self, phis):\n",
    "        n = self.samples\n",
    "        p = self.points\n",
    "        k = len(phis)\n",
    "        rhos = torch.zeros((k, n, p), device=\"cpu\")\n",
    "        lengths = torch.zeros(k)\n",
    "        self_kernels = torch.zeros((k, 1))\n",
    "        for i, phi in enumerate(phis):\n",
    "            if self.boolean:\n",
    "                rho = phi.boolean(self.signals, evaluate_at_all_times=True).float()\n",
    "                rho[rho == 0.0] = -1.0\n",
    "            else:\n",
    "                rho = phi.quantitative(self.signals, evaluate_at_all_times=True)\n",
    "            actual_p = rho.size()[2]\n",
    "            rho = rho.reshape(n, actual_p).cpu()\n",
    "            rhos[i, :, :actual_p] = rho\n",
    "            lengths[i] = actual_p\n",
    "            self_kernels[i] = torch.tensordot(\n",
    "                rho.reshape(1, n, -1), rho.reshape(1, n, -1), dims=[[1, 2], [1, 2]]\n",
    "            ) / (actual_p * n)\n",
    "        return rhos, self_kernels, lengths\n",
    "\n",
    "    def _compute_robustness_no_time(self, phis):\n",
    "        n = self.samples\n",
    "        k = len(phis)\n",
    "        rhos = torch.zeros((k, n), device=self.traj_measure.device)\n",
    "        self_kernels = torch.zeros((k, 1), device=self.traj_measure.device)\n",
    "        for i, phi in enumerate(phis):\n",
    "            if self.boolean:\n",
    "                rho = phi.boolean(self.signals, evaluate_at_all_times=False).float()\n",
    "                rho[rho == 0.0] = -1.0\n",
    "            else:\n",
    "                rho = phi.quantitative(self.signals, evaluate_at_all_times=False)\n",
    "            self_kernels[i] = rho.dot(rho) / n\n",
    "            rhos[i, :] = rho\n",
    "        return rhos, self_kernels\n",
    "\n",
    "    def _compute_kernel_time(self, rhos1, rhos2, selfk1, selfk2, len1, len2):\n",
    "        kernel_matrix = torch.tensordot(rhos1, rhos2, [[1, 2], [1, 2]])\n",
    "        length_normalizer = self._compute_trajectory_length_normalizer(len1, len2)\n",
    "        kernel_matrix = kernel_matrix * length_normalizer / self.samples\n",
    "        if self.normalize:\n",
    "            kernel_matrix = self._normalize(kernel_matrix, selfk1, selfk2)\n",
    "        if self.exp_kernel:\n",
    "            kernel_matrix = self._exponentiate(kernel_matrix, selfk1, selfk2)\n",
    "        return kernel_matrix\n",
    "\n",
    "    def _compute_kernel_no_time(self, rhos1, rhos2, selfk1, selfk2):\n",
    "        kernel_matrix = torch.tensordot(rhos1, rhos2, [[1], [1]])\n",
    "        kernel_matrix = kernel_matrix / self.samples\n",
    "        if self.normalize:\n",
    "            kernel_matrix = self._normalize(kernel_matrix, selfk1, selfk2)\n",
    "        if self.exp_kernel:\n",
    "            kernel_matrix = self._exponentiate(kernel_matrix, selfk1, selfk2)\n",
    "        return kernel_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize(kernel_matrix, selfk1, selfk2):\n",
    "        normalize = torch.sqrt(torch.matmul(selfk1, torch.transpose(selfk2, 0, 1)))\n",
    "        kernel_matrix = kernel_matrix / normalize\n",
    "        return kernel_matrix\n",
    "\n",
    "    def _exponentiate(self, kernel_matrix, selfk1, selfk2, sigma2=None):\n",
    "        if sigma2 is None:\n",
    "            sigma2 = self.sigma2\n",
    "        if self.normalize:\n",
    "            # selfk is (1.0^2 + 1.0^2)\n",
    "            selfk = 2.0\n",
    "        else:\n",
    "            k1 = selfk1.size()[0]\n",
    "            k2 = selfk2.size()[0]\n",
    "            selfk = (selfk1 * selfk1).repeat(1, k2) + torch.transpose(\n",
    "                selfk2 * selfk2, 0, 1\n",
    "            ).repeat(k1, 1)\n",
    "        return torch.exp(-(selfk - 2 * kernel_matrix) / (2 * sigma2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_trajectory_length_normalizer(len1, len2):\n",
    "        k1 = len1.size()[0]\n",
    "        k2 = len2.size()[0]\n",
    "        y1 = len1.reshape(-1, 1)\n",
    "        y1 = y1.repeat(1, k2)\n",
    "        y2 = len2.repeat(k1, 1)\n",
    "        return 1.0 / torch.min(y1, y2)\n",
    "\n",
    "\n",
    "class GramMatrix:\n",
    "    def __init__(self, kernel, formulae, store_robustness=True, sample=False, sampler=None, bag_size=None):\n",
    "        self.kernel = kernel\n",
    "        self.formulae_list = formulae\n",
    "        # if kernel is computed from robustness at time zero only,\n",
    "        # we store the robustness for each formula and each sample\n",
    "        # to speed up computation later\n",
    "        self.store_robustness = store_robustness\n",
    "        self.dim = len(self.formulae_list) if not bag_size else int(bag_size)\n",
    "        self.sample = sample  # whether to generate formulae in a controlled manner\n",
    "        if self.sample:\n",
    "            self.t = 0.99 if self.kernel.boolean else 0.85\n",
    "        self.sampler = sampler  # stl formulae generator\n",
    "        self._compute_gram_matrix()\n",
    "\n",
    "    def _compute_gram_matrix(self):\n",
    "        if self.sample:\n",
    "            gram = torch.zeros(self.dim, self.dim)\n",
    "            rhos = torch.zeros((self.dim, self.kernel.samples), device=self.kernel.traj_measure.device) if \\\n",
    "                not self.kernel.integrate_time else torch.zeros((self.dim, self.kernel.samples, self.kernel.points),\n",
    "                                                                device=self.kernel.traj_measure.device)\n",
    "            lengths = torch.zeros(self.dim) if self.kernel.integrate_time else np.zeros(self.dim)\n",
    "            kernels = torch.zeros((self.dim, 1), device=self.kernel.traj_measure.device)\n",
    "            phis = [self.sampler.sample(nvars=self.kernel.varn)]\n",
    "            gram[0, :1], rhos[0], kernels[0, :], lengths[0] = self.kernel.compute_bag(phis, return_robustness=True)\n",
    "            while len(phis) < self.dim:\n",
    "                i = len(phis)\n",
    "                phi = self.sampler.sample(nvars=self.kernel.varn)\n",
    "                gram[i, :i], rhos[i], kernels[i, :], lengths[i] = self.kernel.compute_one_from_robustness(\n",
    "                    phi, rhos[:i, :], kernels[:i, :], lengths[:i], return_robustness=True)\n",
    "                if torch.sum(gram[i, :i + 1] >= self.t) < 3:\n",
    "                    phis.append(phi)\n",
    "                    gram[:i, i] = gram[i, :i]\n",
    "                    gram[i, i] = kernels[i, :]\n",
    "\n",
    "            self.formulae_list = phis\n",
    "            self.gram = gram.cpu()\n",
    "            self.robustness = rhos if self.store_robustness else None\n",
    "            self.self_kernels = kernels if self.store_robustness else None\n",
    "            self.robustness_lengths = lengths if self.store_robustness else None\n",
    "        else:\n",
    "            if self.store_robustness:\n",
    "                k_matrix, rhos, selfk, len0 = self.kernel.compute_bag(\n",
    "                    self.formulae_list, return_robustness=True\n",
    "                )\n",
    "                self.gram = k_matrix\n",
    "                self.robustness = rhos\n",
    "                self.self_kernels = selfk\n",
    "                self.robustness_lengths = len0\n",
    "            else:\n",
    "                self.gram = self.kernel.compute_bag(\n",
    "                    self.formulae_list, return_robustness=False\n",
    "                )\n",
    "                self.robustness = None\n",
    "                self.self_kernels = None\n",
    "                self.robustness_lengths = None\n",
    "\n",
    "    def compute_kernel_vector(self, phi):\n",
    "        if self.store_robustness:\n",
    "            return self.kernel.compute_one_from_robustness(\n",
    "                phi, self.robustness, self.self_kernels, self.robustness_lengths\n",
    "            )\n",
    "        else:\n",
    "            return self.kernel.compute_one_bag(phi, self.formulae_list)\n",
    "\n",
    "    def compute_bag_kernel_vector(self, phis, generate_phis=False, bag_size=None):\n",
    "        if generate_phis:\n",
    "            gram_test = torch.zeros(bag_size, self.dim)  # self.dim, bag_size\n",
    "            rhos_test = torch.zeros((bag_size, self.kernel.samples), device=self.kernel.traj_measure.device) if \\\n",
    "                not self.kernel.integrate_time else torch.zeros((bag_size, self.kernel.samples, self.kernel.points),\n",
    "                                                                device=self.kernel.traj_measure.device)\n",
    "            lengths_test = torch.zeros(bag_size) if self.kernel.integrate_time else np.zeros(bag_size)\n",
    "            kernels_test = torch.zeros((bag_size, 1), device=self.kernel.traj_measure.device)\n",
    "            phi_test = []\n",
    "            while len(phi_test) < bag_size:\n",
    "                i = len(phi_test)\n",
    "                phi = self.sampler.sample(nvars=self.kernel.varn)\n",
    "                if self.store_robustness:\n",
    "                    gram_test[i, :], rhos_test[i], kernels_test[i, :], lengths_test[i] = \\\n",
    "                        self.kernel.compute_one_from_robustness(phi, self.robustness, self.self_kernels,\n",
    "                                                                self.robustness_lengths, return_robustness=True)\n",
    "                else:\n",
    "                    gram_test[i, :], rhos_test[i], _, kernels_test[i, :], _, lengths_test[i], _ = \\\n",
    "                        self.kernel.compute_one_bag(phi, self.formulae_list, return_robustness=True)\n",
    "                if not ((rhos_test[i] > 0).all() or (rhos_test[i] < 0).all()):\n",
    "                    phi_test.append(phi)\n",
    "            return phi_test, gram_test.cpu()\n",
    "        else:\n",
    "            if self.store_robustness:\n",
    "                return self.kernel.compute_bag_from_robustness(\n",
    "                    phis, self.robustness, self.self_kernels, self.robustness_lengths\n",
    "                )\n",
    "            else:\n",
    "                return self.kernel.compute_bag_bag(phis, self.formulae_list)\n",
    "\n",
    "    def invert_regularized(self, alpha):\n",
    "        regularizer = abs(pow(10, alpha)) * torch.eye(self.dim)\n",
    "        return torch.inverse(self.gram + regularizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849f0340-cc7d-4c5d-82c1-9bf7f6508a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "# Custom types\n",
    "realnum = Union[float, int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38b77e3-0dba-46a6-8140-b1f5de7c5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "    return x\n",
    "\n",
    "\n",
    "def dump_pickle(name, thing):\n",
    "    with open(name + '.pickle', 'wb') as f:\n",
    "        pickle.dump(thing, f)\n",
    "\n",
    "\n",
    "def set_time_thresholds(st):\n",
    "    unbound, right_unbound = [True, False]\n",
    "    left_time_bound, right_time_bound = [0, 0]\n",
    "    if st[-1] == ']':\n",
    "        unbound = False\n",
    "        time_thresholds = st[st.index('[')+1:-1].split(\",\")\n",
    "        left_time_bound = int(time_thresholds[0])\n",
    "        if time_thresholds[1] == 'inf':\n",
    "            right_unbound = True\n",
    "        else:\n",
    "            right_time_bound = int(time_thresholds[1])-1\n",
    "    return unbound, right_unbound, left_time_bound, right_time_bound\n",
    "\n",
    "\n",
    "def from_string_to_formula(st):\n",
    "    root_arity = 2 if st.startswith('(') else 1\n",
    "    st_split = st.split()\n",
    "    if root_arity <= 1:\n",
    "        root_op_str = copy.deepcopy(st_split[0])\n",
    "        if root_op_str.startswith('x'):\n",
    "            atom_sign = True if st_split[1] == '<=' else False\n",
    "            root_phi = Atom(var_index=int(st_split[0][2]), lte=atom_sign, threshold=float(st_split[2]))\n",
    "            return root_phi\n",
    "        else:\n",
    "            assert (root_op_str.startswith('not') or root_op_str.startswith('eventually')\n",
    "                    or root_op_str.startswith('always'))\n",
    "            current_st = copy.deepcopy(st_split[2:-1])\n",
    "            if root_op_str == 'not':\n",
    "                root_phi = Not(child=from_string_to_formula(' '.join(current_st)))\n",
    "            elif root_op_str.startswith('eventually'):\n",
    "                unbound, right_unbound, left_time_bound, right_time_bound = set_time_thresholds(root_op_str)\n",
    "                root_phi = Eventually(child=from_string_to_formula(' '.join(current_st)), unbound=unbound,\n",
    "                                      right_unbound=right_unbound, left_time_bound=left_time_bound,\n",
    "                                      right_time_bound=right_time_bound)\n",
    "            else:\n",
    "                unbound, right_unbound, left_time_bound, right_time_bound = set_time_thresholds(root_op_str)\n",
    "                root_phi = Globally(child=from_string_to_formula(' '.join(current_st)), unbound=unbound,\n",
    "                                    right_unbound=right_unbound, left_time_bound=left_time_bound,\n",
    "                                    right_time_bound=right_time_bound)\n",
    "    else:\n",
    "        # 1 - delete everything which is contained in other sets of parenthesis (if any)\n",
    "        current_st = copy.deepcopy(st_split[1:-1])\n",
    "        if '(' in current_st:\n",
    "            par_queue = deque()\n",
    "            par_idx_list = []\n",
    "            for i, sub in enumerate(current_st):\n",
    "                if sub == '(':\n",
    "                    par_queue.append(i)\n",
    "                elif sub == ')':\n",
    "                    par_idx_list.append(tuple([par_queue.pop(), i]))\n",
    "            # open_par_idx, close_par_idx = [current_st.index(p) for p in ['(', ')']]\n",
    "            # union of parentheses range --> from these we may extract the substrings to be the children!!!\n",
    "            children_range = []\n",
    "            for begin, end in sorted(par_idx_list):\n",
    "                if children_range and children_range[-1][1] >= begin - 1:\n",
    "                    children_range[-1][1] = max(children_range[-1][1], end)\n",
    "                else:\n",
    "                    children_range.append([begin, end])\n",
    "            n_children = len(children_range)\n",
    "            assert (n_children in [1, 2])\n",
    "            if n_children == 1:\n",
    "                # one of the children is a variable --> need to individuate it\n",
    "                var_child_idx = 1 if children_range[0][0] <= 1 else 0  # 0 is left child, 1 is right child\n",
    "                if children_range[0][0] != 0 and current_st[children_range[0][0] - 1][0:2] in ['no', 'ev', 'al']:\n",
    "                    children_range[0][0] -= 1\n",
    "                left_child_str = current_st[:3] if var_child_idx == 0 else \\\n",
    "                    current_st[children_range[0][0]:children_range[0][1] + 1]\n",
    "                right_child_str = current_st[-3:] if var_child_idx == 1 else \\\n",
    "                    current_st[children_range[0][0]:children_range[0][1] + 1]\n",
    "                root_op_str = current_st[children_range[0][1] + 1] if var_child_idx == 1 else \\\n",
    "                    current_st[children_range[0][0] - 1]\n",
    "                assert (root_op_str[:2] in ['an', 'or', 'un'])\n",
    "            else:\n",
    "                if children_range[0][0] != 0 and current_st[children_range[0][0] - 1][0:2] in ['no', 'ev', 'al']:\n",
    "                    children_range[0][0] -= 1\n",
    "                if current_st[children_range[1][0] - 1][0:2] in ['no', 'ev', 'al']:\n",
    "                    children_range[1][0] -= 1\n",
    "                # if there are two children, with parentheses, the element in the middle is the root\n",
    "                root_op_str = current_st[children_range[0][1] + 1]\n",
    "                assert (root_op_str[:2] in ['an', 'or', 'un'])\n",
    "                left_child_str = current_st[children_range[0][0]:children_range[0][1] + 1]\n",
    "                right_child_str = current_st[children_range[1][0]:children_range[1][1] + 1]\n",
    "        else:\n",
    "            # no parentheses means that both children are variables\n",
    "            left_child_str = current_st[:3]\n",
    "            right_child_str = current_st[-3:]\n",
    "            root_op_str = current_st[3]\n",
    "        left_child_str = ' '.join(left_child_str)\n",
    "        right_child_str = ' '.join(right_child_str)\n",
    "        if root_op_str == 'and':\n",
    "            root_phi = And(left_child=from_string_to_formula(left_child_str),\n",
    "                           right_child=from_string_to_formula(right_child_str))\n",
    "        elif root_op_str == 'or':\n",
    "            root_phi = Or(left_child=from_string_to_formula(left_child_str),\n",
    "                          right_child=from_string_to_formula(right_child_str))\n",
    "        else:\n",
    "            unbound, right_unbound, left_time_bound, right_time_bound = set_time_thresholds(root_op_str)\n",
    "            root_phi = Until(left_child=from_string_to_formula(left_child_str),\n",
    "                             right_child=from_string_to_formula(right_child_str),\n",
    "                             unbound=unbound, right_unbound=right_unbound, left_time_bound=left_time_bound,\n",
    "                             right_time_bound=right_time_bound)\n",
    "    return root_phi\n",
    "\n",
    "\n",
    "def scale_trajectories(traj):\n",
    "    traj_min = torch.min(torch.min(traj, dim=0)[0], dim=0)[0]\n",
    "    traj_max = torch.max(torch.max(traj, dim=0)[0], dim=0)[0]\n",
    "    scaled_traj = -1 + 2*(traj - traj_min) / (traj_max - traj_min)\n",
    "    return scaled_traj\n",
    "\n",
    "\n",
    "def standardize_trajectories(traj_data, n_var):\n",
    "    means, stds = [[] for _ in range(2)]\n",
    "    for i in range(n_var):\n",
    "        means.append(torch.mean(traj_data[:, i, :]))\n",
    "        stds.append(torch.std(traj_data[:, i, :]))\n",
    "    for i in range(n_var):\n",
    "        traj_data[:, i, :] = (traj_data[:, i, :] - means[i]) / stds[i]\n",
    "    return traj_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5901464b-2efd-4caf-93bd-faa17a4d48cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "\n",
    "class Measure:\n",
    "    def sample(self, samples=100000, varn=2, points=100):\n",
    "        # Must be overridden\n",
    "        pass\n",
    "\n",
    "\n",
    "class BaseMeasure(Measure):\n",
    "    def __init__(\n",
    "        self, mu0=0.0, sigma0=1.0, mu1=0.0, sigma1=1.0, q=0.1, q0=0.5, device=\"cpu\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        mu0 : mean of normal distribution of initial state, optional\n",
    "            The default is 0.0.\n",
    "        sigma0 : standard deviation of normal distribution of initial state, optional\n",
    "            The default is 1.0.\n",
    "        mu1 : DOUBLE, optional\n",
    "            mean of normal distribution of total variation. The default is 0.0.\n",
    "        sigma1 : standard deviation of normal distribution of total variation, optional\n",
    "            The default is 1.0.\n",
    "        q : DOUBLE, optional\n",
    "            probability of change of sign in derivative. The default is 0.1.\n",
    "        q0 : DOUBLE, optional\n",
    "            probability of initial sign of  derivative. The default is 0.5.\n",
    "        device : 'cpu' or 'cuda', optional\n",
    "            device on which to run the algorithm. The default is 'cpu'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.mu0 = mu0\n",
    "        self.sigma0 = sigma0\n",
    "        self.mu1 = mu1\n",
    "        self.sigma1 = sigma1\n",
    "        self.q = q\n",
    "        self.q0 = q0\n",
    "        self.device = device\n",
    "\n",
    "    def sample(self, samples=100000, varn=2, points=100):\n",
    "        \"\"\"\n",
    "        Samples a set of trajectories from the basic measure space, with parameters\n",
    "        passed to the sampler\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        points : INT, optional\n",
    "            number of points per trajectory, including initial one. The default is 1000.\n",
    "        samples : INT, optional\n",
    "            number of trajectories. The default is 100000.\n",
    "        varn : INT, optional\n",
    "            number of variables per trajectory. The default is 2.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        signal : samples x varn x points double pytorch tensor\n",
    "            The sampled signals.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"GPU card or CUDA library not available!\")\n",
    "\n",
    "        # generate unif RN\n",
    "        signal = torch.rand(samples, varn, points, device=self.device)\n",
    "        # first point is special - set to zero for the moment, and set one point to 1\n",
    "        signal[:, :, 0] = 0.0\n",
    "        signal[:, :, -1] = 1.0\n",
    "        # sorting each trajectory\n",
    "        signal, _ = torch.sort(signal, 2)\n",
    "        # computing increments and storing them in points 1 to end\n",
    "        signal[:, :, 1:] = signal[:, :, 1:] - signal[:, :, :-1]\n",
    "        # generate initial state, according to a normal distribution\n",
    "        signal[:, :, 0] = self.mu0 + self.sigma0 * torch.randn(signal[:, :, 0].size())\n",
    "\n",
    "        # sampling change signs from bernoulli in -1, 1\n",
    "        derivs = (1 - self.q) * torch.ones(samples, varn, points, device=self.device)\n",
    "        derivs = 2 * torch.bernoulli(derivs) - 1\n",
    "        # sampling initial derivative\n",
    "        derivs[:, :, 0] = self.q0\n",
    "        derivs[:, :, 0] = 2 * torch.bernoulli(derivs[:, :, 0]) - 1\n",
    "        # taking the cumulative product along axis 2\n",
    "        derivs = torch.cumprod(derivs, 2)\n",
    "\n",
    "        # sampling total variation\n",
    "        totvar = torch.pow(\n",
    "            self.mu1 + self.sigma1 * torch.randn(samples, varn, 1, device=self.device),\n",
    "            2,\n",
    "         )\n",
    "        # multiplying total variation and derivatives and making initial point non-invasive\n",
    "        derivs = derivs * totvar\n",
    "        derivs[:, :, 0] = 1.0\n",
    "\n",
    "        # computing trajectories by multiplying and then doing a cumulative sum\n",
    "        signal = signal * derivs\n",
    "        signal = torch.cumsum(signal, 2)\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fea104-cc3f-4967-947c-d987a1ff3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensor functions\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def eventually(x: Tensor, time_span: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    STL operator 'eventually' in 1D.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "        Signal\n",
    "    time_span: any numeric type\n",
    "        Timespan duration\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "    A tensor containing the result of the operation.\n",
    "    \"\"\"\n",
    "    return F.max_pool1d(x, kernel_size=time_span, stride=1)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Abstract node class for STL semantics tree.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Must be overloaded.\n",
    "        pass\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        # Must be overloaded.\n",
    "        pass\n",
    "\n",
    "    def boolean(self, x: Tensor, evaluate_at_all_times: bool = False) -> Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates the boolean semantics at the node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor, of size N_samples x N_vars x N_sampling_points\n",
    "            The input signals, stored as a batch tensor with trhee dimensions.\n",
    "        evaluate_at_all_times: bool\n",
    "            Whether to evaluate the semantics at all times (True) or\n",
    "            just at t=0 (False).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        A tensor with the boolean semantics for the node.\n",
    "        \"\"\"\n",
    "        z: Tensor = self._boolean(x)\n",
    "        if evaluate_at_all_times:\n",
    "            return z\n",
    "        else:\n",
    "            return self._extract_semantics_at_time_zero(z)\n",
    "\n",
    "    def quantitative(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        normalize: bool = False,\n",
    "        evaluate_at_all_times: bool = False,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Evaluates the quantitative semantics at the node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor, of size N_samples x N_vars x N_sampling_points\n",
    "            The input signals, stored as a batch tensor with three dimensions.\n",
    "        normalize: bool\n",
    "            Whether the measure of robustness if normalized (True) or\n",
    "            not (False). Currently not in use.\n",
    "        evaluate_at_all_times: bool\n",
    "            Whether to evaluate the semantics at all times (True) or\n",
    "            just at t=0 (False).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "        A tensor with the quantitative semantics for the node.\n",
    "        \"\"\"\n",
    "        z: Tensor = self._quantitative(x, normalize)\n",
    "        if evaluate_at_all_times:\n",
    "            return z\n",
    "        else:\n",
    "            return self._extract_semantics_at_time_zero(z)\n",
    "\n",
    "    def set_normalizing_flag(self, value: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Setter for the 'normalization of robustness of the formula' flag.\n",
    "        Currently not in use.\n",
    "        \"\"\"\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        \"\"\"Returns time depth of bounded temporal operators only.\"\"\"\n",
    "        # Must be overloaded.\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        \"\"\"Private method equivalent to public one for inner call.\"\"\"\n",
    "        # Must be overloaded.\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Private method equivalent to public one for inner call.\"\"\"\n",
    "        # Must be overloaded.\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_semantics_at_time_zero(x: Tensor) -> Tensor:\n",
    "        \"\"\"Extrapolates the vector of truth values at time zero\"\"\"\n",
    "        return torch.reshape(x[:, 0, 0], (-1,))\n",
    "\n",
    "\n",
    "class Atom(Node):\n",
    "    \"\"\"Atomic formula node; for now of the form X<=t or X>=t\"\"\"\n",
    "\n",
    "    def __init__(self, var_index: int, threshold: realnum, lte: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self.var_index: int = var_index\n",
    "        self.threshold: realnum = threshold\n",
    "        self.lte: bool = lte\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s: str = (\n",
    "            \"x_\"\n",
    "            + str(self.var_index)\n",
    "            + (\" <= \" if self.lte else \" >= \")\n",
    "            + str(round(self.threshold, 4))\n",
    "        )\n",
    "        return s\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        return 0\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        # extract tensor of the same dimension as data, but with only one variable\n",
    "        xj: Tensor = x[:, self.var_index, :]\n",
    "        xj: Tensor = xj.view(xj.size()[0], 1, -1)\n",
    "        if self.lte:\n",
    "            z: Tensor = torch.le(xj, self.threshold)\n",
    "        else:\n",
    "            z: Tensor = torch.ge(xj, self.threshold)\n",
    "        return z\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        # extract tensor of the same dimension as data, but with only one variable\n",
    "        xj: Tensor = x[:, self.var_index, :]\n",
    "        xj: Tensor = xj.view(xj.size()[0], 1, -1)\n",
    "        if self.lte:\n",
    "            z: Tensor = -xj + self.threshold\n",
    "        else:\n",
    "            z: Tensor = xj - self.threshold\n",
    "        if normalize:\n",
    "            z: Tensor = torch.tanh(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Not(Node):\n",
    "    \"\"\"Negation node.\"\"\"\n",
    "\n",
    "    def __init__(self, child: Node) -> None:\n",
    "        super().__init__()\n",
    "        self.child: Node = child\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s: str = \"not ( \" + self.child.__str__() + \" )\"\n",
    "        return s\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        return self.child.time_depth()\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        z: Tensor = ~self.child._boolean(x)\n",
    "        return z\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        z: Tensor = -self.child._quantitative(x, normalize)\n",
    "        return z\n",
    "\n",
    "\n",
    "class And(Node):\n",
    "    \"\"\"Conjunction node.\"\"\"\n",
    "\n",
    "    def __init__(self, left_child: Node, right_child: Node) -> None:\n",
    "        super().__init__()\n",
    "        self.left_child: Node = left_child\n",
    "        self.right_child: Node = right_child\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s: str = (\n",
    "            \"( \"\n",
    "            + self.left_child.__str__()\n",
    "            + \" and \"\n",
    "            + self.right_child.__str__()\n",
    "            + \" )\"\n",
    "        )\n",
    "        return s\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        return max(self.left_child.time_depth(), self.right_child.time_depth())\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        z1: Tensor = self.left_child._boolean(x)\n",
    "        z2: Tensor = self.right_child._boolean(x)\n",
    "        size: int = min(z1.size()[2], z2.size()[2])\n",
    "        z1: Tensor = z1[:, :, :size]\n",
    "        z2: Tensor = z2[:, :, :size]\n",
    "        z: Tensor = torch.logical_and(z1, z2)\n",
    "        return z\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        z1: Tensor = self.left_child._quantitative(x, normalize)\n",
    "        z2: Tensor = self.right_child._quantitative(x, normalize)\n",
    "        size: int = min(z1.size()[2], z2.size()[2])\n",
    "        z1: Tensor = z1[:, :, :size]\n",
    "        z2: Tensor = z2[:, :, :size]\n",
    "        z: Tensor = torch.min(z1, z2)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Or(Node):\n",
    "    \"\"\"Disjunction node.\"\"\"\n",
    "\n",
    "    def __init__(self, left_child: Node, right_child: Node) -> None:\n",
    "        super().__init__()\n",
    "        self.left_child: Node = left_child\n",
    "        self.right_child: Node = right_child\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s: str = (\n",
    "            \"( \"\n",
    "            + self.left_child.__str__()\n",
    "            + \" or \"\n",
    "            + self.right_child.__str__()\n",
    "            + \" )\"\n",
    "        )\n",
    "        return s\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        return max(self.left_child.time_depth(), self.right_child.time_depth())\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        z1: Tensor = self.left_child._boolean(x)\n",
    "        z2: Tensor = self.right_child._boolean(x)\n",
    "        size: int = min(z1.size()[2], z2.size()[2])\n",
    "        z1: Tensor = z1[:, :, :size]\n",
    "        z2: Tensor = z2[:, :, :size]\n",
    "        z: Tensor = torch.logical_or(z1, z2)\n",
    "        return z\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        z1: Tensor = self.left_child._quantitative(x, normalize)\n",
    "        z2: Tensor = self.right_child._quantitative(x, normalize)\n",
    "        size: int = min(z1.size()[2], z2.size()[2])\n",
    "        z1: Tensor = z1[:, :, :size]\n",
    "        z2: Tensor = z2[:, :, :size]\n",
    "        z: Tensor = torch.max(z1, z2)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Globally(Node):\n",
    "    \"\"\"Globally node.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        child: Node,\n",
    "        unbound: bool = False,\n",
    "        right_unbound: bool = False,\n",
    "        left_time_bound: int = 0,\n",
    "        right_time_bound: int = 1,\n",
    "        adapt_unbound: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.child: Node = child\n",
    "        self.unbound: bool = unbound\n",
    "        self.right_unbound: bool = right_unbound\n",
    "        self.left_time_bound: int = left_time_bound\n",
    "        self.right_time_bound: int = right_time_bound + 1\n",
    "        self.adapt_unbound: bool = adapt_unbound\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s_left = \"[\" + str(self.left_time_bound) + \",\"\n",
    "        s_right = str(self.right_time_bound) if not self.right_unbound else \"inf\"\n",
    "        s0: str = s_left + s_right + \"]\" if not self.unbound else \"\"\n",
    "        s: str = \"always\" + s0 + \" ( \" + self.child.__str__() + \" )\"\n",
    "        return s\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        if self.unbound:\n",
    "            return self.child.time_depth()\n",
    "        elif self.right_unbound:\n",
    "            return self.child.time_depth() + self.left_time_bound\n",
    "        else:\n",
    "            # diff = torch.le(torch.tensor([self.left_time_bound]), 0).float()\n",
    "            return self.child.time_depth() + self.right_time_bound - 1\n",
    "            # (self.right_time_bound - self.left_time_bound + 1) - diff\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        z1: Tensor = self.child._boolean(x[:, :, self.left_time_bound:])  # nested temporal parameters\n",
    "        # z1 = z1[:, :, self.left_time_bound:]\n",
    "        if self.unbound or self.right_unbound:\n",
    "            if self.adapt_unbound:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.cummin(torch.flip(z1, [2]), dim=2)\n",
    "                z: Tensor = torch.flip(z, [2])\n",
    "            else:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.min(z1, 2, keepdim=True)\n",
    "        else:\n",
    "            z: Tensor = torch.ge(1.0 - eventually((~z1).double(), self.right_time_bound - self.left_time_bound), 0.5)\n",
    "        return z\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        z1: Tensor = self.child._quantitative(x[:, :, self.left_time_bound:], normalize)\n",
    "        # z1 = z1[:, :, self.left_time_bound:]\n",
    "        if self.unbound or self.right_unbound:\n",
    "            if self.adapt_unbound:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.cummin(torch.flip(z1, [2]), dim=2)\n",
    "                z: Tensor = torch.flip(z, [2])\n",
    "            else:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.min(z1, 2, keepdim=True)\n",
    "        else:\n",
    "            z: Tensor = -eventually(-z1, self.right_time_bound - self.left_time_bound)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Eventually(Node):\n",
    "    \"\"\"Eventually node.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        child: Node,\n",
    "        unbound: bool = False,\n",
    "        right_unbound: bool = False,\n",
    "        left_time_bound: int = 0,\n",
    "        right_time_bound: int = 1,\n",
    "        adapt_unbound: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.child: Node = child\n",
    "        self.unbound: bool = unbound\n",
    "        self.right_unbound: bool = right_unbound\n",
    "        self.left_time_bound: int = left_time_bound\n",
    "        self.right_time_bound: int = right_time_bound + 1\n",
    "        self.adapt_unbound: bool = adapt_unbound\n",
    "\n",
    "        if (self.unbound is False) and (self.right_unbound is False) and \\\n",
    "                (self.right_time_bound <= self.left_time_bound):\n",
    "            raise ValueError(\"Temporal thresholds are incorrect: right parameter is higher than left parameter\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s_left = \"[\" + str(self.left_time_bound) + \",\"\n",
    "        s_right = str(self.right_time_bound) if not self.right_unbound else \"inf\"\n",
    "        s0: str = s_left + s_right + \"]\" if not self.unbound else \"\"\n",
    "        s: str = \"eventually\" + s0 + \" ( \" + self.child.__str__() + \" )\"\n",
    "        return s\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        if self.unbound:\n",
    "            return self.child.time_depth()\n",
    "        elif self.right_unbound:\n",
    "            return self.child.time_depth() + self.left_time_bound\n",
    "        else:\n",
    "            # diff = torch.le(torch.tensor([self.left_time_bound]), 0).float()\n",
    "            return self.child.time_depth() + self.right_time_bound - 1\n",
    "            # (self.right_time_bound - self.left_time_bound + 1) - diff\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        z1: Tensor = self.child._boolean(x[:, :, self.left_time_bound:])\n",
    "        if self.unbound or self.right_unbound:\n",
    "            if self.adapt_unbound:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.cummax(torch.flip(z1, [2]), dim=2)\n",
    "                z: Tensor = torch.flip(z, [2])\n",
    "            else:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.max(z1, 2, keepdim=True)\n",
    "        else:\n",
    "            z: Tensor = torch.ge(eventually(z1.double(), self.right_time_bound - self.left_time_bound), 0.5)\n",
    "        return z\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        z1: Tensor = self.child._quantitative(x[:, :, self.left_time_bound:], normalize)\n",
    "        if self.unbound or self.right_unbound:\n",
    "            if self.adapt_unbound:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.cummax(torch.flip(z1, [2]), dim=2)\n",
    "                z: Tensor = torch.flip(z, [2])\n",
    "            else:\n",
    "                z: Tensor\n",
    "                _: Tensor\n",
    "                z, _ = torch.max(z1, 2, keepdim=True)\n",
    "        else:\n",
    "            z: Tensor = eventually(z1, self.right_time_bound - self.left_time_bound)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Until(Node):\n",
    "    \"\"\"Until node.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        left_child: Node,\n",
    "        right_child: Node,\n",
    "        unbound: bool = False,\n",
    "        right_unbound: bool = False,\n",
    "        left_time_bound: int = 0,\n",
    "        right_time_bound: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.left_child: Node = left_child\n",
    "        self.right_child: Node = right_child\n",
    "        self.unbound: bool = unbound\n",
    "        self.right_unbound: bool = right_unbound\n",
    "        self.left_time_bound: int = left_time_bound\n",
    "        self.right_time_bound: int = right_time_bound + 1\n",
    "\n",
    "        if (self.unbound is False) and (self.right_unbound is False) and \\\n",
    "                (self.right_time_bound <= self.left_time_bound):\n",
    "            raise ValueError(\"Temporal thresholds are incorrect: right parameter is higher than left parameter\")\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s_left = \"[\" + str(self.left_time_bound) + \",\"\n",
    "        s_right = str(self.right_time_bound) if not self.right_unbound else \"inf\"\n",
    "        s0: str = s_left + s_right + \"]\" if not self.unbound else \"\"\n",
    "        s: str = \"( \" + self.left_child.__str__() + \" until\" + s0 + \" \" + self.right_child.__str__() + \" )\"\n",
    "        return s\n",
    "\n",
    "    def time_depth(self) -> int:\n",
    "        sum_children_depth: int = self.left_child.time_depth() + self.right_child.time_depth()\n",
    "        if self.unbound:\n",
    "            return sum_children_depth\n",
    "        elif self.right_unbound:\n",
    "            return sum_children_depth + self.left_time_bound\n",
    "        else:\n",
    "            # diff = torch.le(torch.tensor([self.left_time_bound]), 0).float()\n",
    "            return sum_children_depth + self.right_time_bound - 1\n",
    "            # (self.right_time_bound - self.left_time_bound + 1) - diff\n",
    "\n",
    "    def _boolean(self, x: Tensor) -> Tensor:\n",
    "        if self.unbound:\n",
    "            z1: Tensor = self.left_child._boolean(x)\n",
    "            z2: Tensor = self.right_child._boolean(x)\n",
    "            size: int = min(z1.size()[2], z2.size()[2])\n",
    "            z1: Tensor = z1[:, :, :size]\n",
    "            z2: Tensor = z2[:, :, :size]\n",
    "            z1_rep = torch.repeat_interleave(z1.unsqueeze(2), z1.unsqueeze(2).shape[-1], 2)\n",
    "            z1_tril = torch.tril(z1_rep.transpose(2, 3), diagonal=-1)\n",
    "            z1_triu = torch.triu(z1_rep)\n",
    "            z1_def = torch.cummin(z1_tril + z1_triu, dim=3)[0]\n",
    "\n",
    "            z2_rep = torch.repeat_interleave(z2.unsqueeze(2), z2.unsqueeze(2).shape[-1], 2)\n",
    "            z2_tril = torch.tril(z2_rep.transpose(2, 3), diagonal=-1)\n",
    "            z2_triu = torch.triu(z2_rep)\n",
    "            z2_def = z2_tril + z2_triu\n",
    "            z: Tensor = torch.max(torch.min(torch.cat([z1_def.unsqueeze(-1), z2_def.unsqueeze(-1)], dim=-1), dim=-1)[0],\n",
    "                                  dim=-1)[0]\n",
    "        elif self.right_unbound:\n",
    "            timed_until: Node = And(Globally(self.left_child, left_time_bound=0, right_time_bound=self.left_time_bound),\n",
    "                                    And(Eventually(self.right_child, right_unbound=True,\n",
    "                                                   left_time_bound=self.left_time_bound),\n",
    "                                        Eventually(Until(self.left_child, self.right_child, unbound=True),\n",
    "                                                   left_time_bound=self.left_time_bound, right_unbound=True)))\n",
    "            z: Tensor = timed_until._boolean(x)\n",
    "        else:\n",
    "            timed_until: Node = And(Globally(self.left_child, left_time_bound=0, right_time_bound=self.left_time_bound),\n",
    "                                    And(Eventually(self.right_child, left_time_bound=self.left_time_bound,\n",
    "                                                   right_time_bound=self.right_time_bound - 1),\n",
    "                                        Eventually(Until(self.left_child, self.right_child, unbound=True),\n",
    "                                                   left_time_bound=self.left_time_bound, right_unbound=True)))\n",
    "            z: Tensor = timed_until._boolean(x)\n",
    "        return z\n",
    "\n",
    "    def _quantitative(self, x: Tensor, normalize: bool = False) -> Tensor:\n",
    "        if self.unbound:\n",
    "            z1: Tensor = self.left_child._quantitative(x, normalize)\n",
    "            z2: Tensor = self.right_child._quantitative(x, normalize)\n",
    "            size: int = min(z1.size()[2], z2.size()[2])\n",
    "            z1: Tensor = z1[:, :, :size]\n",
    "            z2: Tensor = z2[:, :, :size]\n",
    "\n",
    "            # z1_rep = torch.repeat_interleave(z1.unsqueeze(2), z1.unsqueeze(2).shape[-1], 2)\n",
    "            # z1_tril = torch.tril(z1_rep.transpose(2, 3), diagonal=-1)\n",
    "            # z1_triu = torch.triu(z1_rep)\n",
    "            # z1_def = torch.cummin(z1_tril + z1_triu, dim=3)[0]\n",
    "\n",
    "            # z2_rep = torch.repeat_interleave(z2.unsqueeze(2), z2.unsqueeze(2).shape[-1], 2)\n",
    "            # z2_tril = torch.tril(z2_rep.transpose(2, 3), diagonal=-1)\n",
    "            # z2_triu = torch.triu(z2_rep)\n",
    "            # z2_def = z2_tril + z2_triu\n",
    "            # z: Tensor = torch.max(torch.min(torch.cat([z1_def.unsqueeze(-1), z2_def.unsqueeze(-1)], dim=-1), dim=-1)[0],\n",
    "            #                       dim=-1)[0]\n",
    "            z: Tensor = torch.cat([torch.max(torch.min(\n",
    "                torch.cat([torch.cummin(z1[:, :, t:].unsqueeze(-1), dim=2)[0], z2[:, :, t:].unsqueeze(-1)], dim=-1),\n",
    "                dim=-1)[0], dim=2, keepdim=True)[0] for t in range(size)], dim=2)\n",
    "        elif self.right_unbound:\n",
    "            timed_until: Node = And(Globally(self.left_child, left_time_bound=0, right_time_bound=self.left_time_bound),\n",
    "                                    And(Eventually(self.right_child, right_unbound=True,\n",
    "                                                   left_time_bound=self.left_time_bound),\n",
    "                                        Eventually(Until(self.left_child, self.right_child, unbound=True),\n",
    "                                                   left_time_bound=self.left_time_bound, right_unbound=True)))\n",
    "            z: Tensor = timed_until._quantitative(x, normalize=normalize)\n",
    "        else:\n",
    "            timed_until: Node = And(Globally(self.left_child, left_time_bound=0, right_time_bound=self.left_time_bound),\n",
    "                                    And(Eventually(self.right_child, left_time_bound=self.left_time_bound,\n",
    "                                                   right_time_bound=self.right_time_bound-1),\n",
    "                                        Eventually(Until(self.left_child, self.right_child, unbound=True),\n",
    "                                                   left_time_bound=self.left_time_bound, right_unbound=True)))\n",
    "            z: Tensor = timed_until._quantitative(x, normalize=normalize)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eab355b-ab43-4108-9684-ede00f9fea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd\n",
    "\n",
    "class StlGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        leaf_prob: float = 0.3,\n",
    "        inner_node_prob: list = None,\n",
    "        threshold_mean: float = 0.0,\n",
    "        threshold_sd: float = 1.0,\n",
    "        unbound_prob: float = 0.1,\n",
    "        right_unbound_prob: float = 0.2,\n",
    "        time_bound_max_range: float = 20,\n",
    "        adaptive_unbound_temporal_ops: bool = True,\n",
    "        max_timespan: int = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        leaf_prob\n",
    "            probability of generating a leaf (always zero for root)\n",
    "        node_types = [\"not\", \"and\", \"or\", \"always\", \"eventually\", \"until\"]\n",
    "            Inner node types\n",
    "        inner_node_prob\n",
    "            probability vector for the different types of internal nodes\n",
    "        threshold_mean\n",
    "        threshold_sd\n",
    "            mean and std for the normal distribution of the thresholds of atoms\n",
    "        unbound_prob\n",
    "            probability of a temporal operator to have a time bound o the type [0,infty]\n",
    "        time_bound_max_range\n",
    "            maximum value of time span of a temporal operator (i.e. max value of t in [0,t])\n",
    "        adaptive_unbound_temporal_ops\n",
    "            if true, unbounded temporal operators are computed from current point to the end of the signal, otherwise\n",
    "            they are evaluated only at time zero.\n",
    "        max_timespan\n",
    "            maximum time depth of a formula.\n",
    "        \"\"\"\n",
    "\n",
    "        # Address the mutability of default arguments\n",
    "        if inner_node_prob is None:\n",
    "            inner_node_prob = [0.166, 0.166, 0.166, 0.17, 0.166, 0.166]\n",
    "\n",
    "        self.leaf_prob = leaf_prob\n",
    "        self.inner_node_prob = inner_node_prob\n",
    "        self.threshold_mean = threshold_mean\n",
    "        self.threshold_sd = threshold_sd\n",
    "        self.unbound_prob = unbound_prob\n",
    "        self.right_unbound_prob = right_unbound_prob\n",
    "        self.time_bound_max_range = time_bound_max_range\n",
    "        self.adaptive_unbound_temporal_ops = adaptive_unbound_temporal_ops\n",
    "        self.node_types = [\"not\", \"and\", \"or\", \"always\", \"eventually\", \"until\"]\n",
    "        self.max_timespan = max_timespan\n",
    "\n",
    "    def sample(self, nvars):\n",
    "        \"\"\"\n",
    "        Samples a random formula with distribution defined in class instance parameters\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        nvars : number of variables of input signals\n",
    "            how many variables the formula is expected to consider.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        TYPE\n",
    "            A random formula.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._sample_internal_node(nvars)\n",
    "\n",
    "    def bag_sample(self, bag_size, nvars):\n",
    "        \"\"\"\n",
    "        Samples a bag of bag_size formulae\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bag_size : INT\n",
    "            number of formulae.\n",
    "        nvars : INT\n",
    "            number of vars in formulae.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a list of formulae.\n",
    "\n",
    "        \"\"\"\n",
    "        formulae = []\n",
    "        for _ in range(bag_size):\n",
    "            phi = self.sample(nvars)\n",
    "            formulae.append(phi)\n",
    "        return formulae\n",
    "\n",
    "    def _sample_internal_node(self, nvars):\n",
    "        # Declare & dummy-assign \"idiom\"\n",
    "        node: Union[None, Node]\n",
    "        node = None\n",
    "        # choose node type\n",
    "        nodetype = rnd.choice(self.node_types, p=self.inner_node_prob)\n",
    "        while True:\n",
    "            if nodetype == \"not\":\n",
    "                n = self._sample_node(nvars)\n",
    "                node = Not(n)\n",
    "            elif nodetype == \"and\":\n",
    "                n1 = self._sample_node(nvars)\n",
    "                n2 = self._sample_node(nvars)\n",
    "                node = And(n1, n2)\n",
    "            elif nodetype == \"or\":\n",
    "                n1 = self._sample_node(nvars)\n",
    "                n2 = self._sample_node(nvars)\n",
    "                node = Or(n1, n2)\n",
    "            elif nodetype == \"always\":\n",
    "                n = self._sample_node(nvars)\n",
    "                unbound, right_unbound, left_time_bound, right_time_bound = self._get_temporal_parameters()\n",
    "                node = Globally(\n",
    "                    n, unbound, right_unbound, left_time_bound, right_time_bound, self.adaptive_unbound_temporal_ops\n",
    "                )\n",
    "            elif nodetype == \"eventually\":\n",
    "                n = self._sample_node(nvars)\n",
    "                unbound, right_unbound, left_time_bound, right_time_bound = self._get_temporal_parameters()\n",
    "                node = Eventually(\n",
    "                    n, unbound, right_unbound, left_time_bound, right_time_bound, self.adaptive_unbound_temporal_ops\n",
    "                )\n",
    "            elif nodetype == \"until\":\n",
    "                n1 = self._sample_node(nvars)\n",
    "                n2 = self._sample_node(nvars)\n",
    "                unbound, right_unbound, left_time_bound, right_time_bound = self._get_temporal_parameters()\n",
    "                node = Until(\n",
    "                    n1, n2, unbound, right_unbound, left_time_bound, right_time_bound\n",
    "                )\n",
    "\n",
    "            if (node is not None) and (node.time_depth() < self.max_timespan):\n",
    "                return node\n",
    "\n",
    "    def _sample_node(self, nvars):\n",
    "        if rnd.rand() < self.leaf_prob:\n",
    "            # sample a leaf\n",
    "            var, thr, lte = self._get_atom(nvars)\n",
    "            return Atom(var, thr, lte)\n",
    "        else:\n",
    "            return self._sample_internal_node(nvars)\n",
    "\n",
    "    def _get_temporal_parameters(self):\n",
    "        if rnd.rand() < self.unbound_prob:\n",
    "            return True, False, 0, 0\n",
    "        elif rnd.rand() < self.right_unbound_prob:\n",
    "            return False, True, rnd.randint(self.time_bound_max_range), 1\n",
    "        else:\n",
    "            left_bound = rnd.randint(self.time_bound_max_range)\n",
    "            return False, False, left_bound, rnd.randint(left_bound, self.time_bound_max_range) + 1\n",
    "\n",
    "    def _get_atom(self, nvars):\n",
    "        variable = rnd.randint(nvars)\n",
    "        lte = rnd.rand() > 0.5\n",
    "        threshold = rnd.normal(self.threshold_mean, self.threshold_sd)\n",
    "        return variable, threshold, lte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0dac7fd6-59d0-4752-ad9f-64c17856d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def anchorGeneration(anchor_path, # path of the generated anchor set\n",
    "                     diff_init = False, # to control whether we want formulae to be semantically different by construction\n",
    "                     embed_dim: int = 30, # embedding dimension, aka number of generated formulae in the anchor set\n",
    "                     n_vars: int = 3, # dimension of the input signal (3D in this case)\n",
    "                     n_phis: int = 10, \n",
    "                     leaf_prob: float = 0.4, # complexity of the generated formula\n",
    "                     cosine_similarity_threshold: float = 0.8 # if two formulae cosine similarity exceeds 0.9, then discard one of the two\n",
    "                    ):\n",
    "    \n",
    "    # initialize STL formula generator\n",
    "    sampler = StlGenerator(leaf_prob)\n",
    "    \n",
    "    # effective anchor set generation\n",
    "    if diff_init:\n",
    "        \n",
    "        # initialize the anchor set with a randomly sampled formula\n",
    "        diff_anchor_set = [sampler.sample(nvars=n_vars)]\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        mu = BaseMeasure(device=device)\n",
    "\n",
    "        # generates a set of random signals working as a tester for the formulae testing\n",
    "        signals = mu.sample(samples=10000, varn=n_vars)\n",
    "\n",
    "        # computes robustness value for the initial set of formulae in the anchor set\n",
    "        anchor_rob_vectors = torch.cat([phi.quantitative(signals, normalize=True).unsqueeze(0) for phi in diff_anchor_set], 0)\n",
    "\n",
    "        while len(diff_anchor_set) < n_phis:\n",
    "            # sample the 'remaining' formulae to reach the desired number of `n_phis` formulae:\n",
    "            candidate_anchors = sampler.bag_sample(n_phis - len(diff_anchor_set), nvars = n_vars)\n",
    "    \n",
    "            # compute robustness of candidate anchor formulae on the same signals as previous anchor set\n",
    "            candidate_robs = torch.cat([phi.quantitative(signals, normalize=True).unsqueeze(0) for phi in candidate_anchors], 0)\n",
    "            \n",
    "            # compute cosine similarity between current anchor set and candidate new formulae\n",
    "            cos_simil = torch.tril(normalize(candidate_robs) @ normalize(anchor_rob_vectors).t(), diagonal=-1)\n",
    "\n",
    "            # check which formulae are similar (i.e. greater cosine similarity then threshold) w.r.t. current anchors\n",
    "            # NOTA: chiedere a gaia se cosine similarities negative vanno ammazzate con un valore assoluto o meno!\n",
    "            similar_idx = [torch.where(cos_simil[r, :] > cosine_similarity_threshold)[0].tolist() for r in range(cos_simil.shape[0])]\n",
    "    \n",
    "            # keep only those who are semantically distant\n",
    "            keep_idx = list(set(np.arange(len(candidate_anchors)).tolist()).difference(set([i for sublist in similar_idx for i in sublist])))\n",
    "            \n",
    "            diff_anchor_set += [copy.deepcopy(candidate_anchors[i]) for i in keep_idx]\n",
    "            \n",
    "            # Convert keep_idx to a tensor on the same device as candidate_robs\n",
    "            keep_idx_tensor = torch.tensor(keep_idx, device=candidate_robs.device)\n",
    "            \n",
    "            # Use index_select to pick the relevant rows\n",
    "            selected_robs = torch.index_select(candidate_robs, 0, keep_idx_tensor)\n",
    "            \n",
    "            # Concatenate on the same device\n",
    "            anchor_rob_vectors = torch.cat([anchor_rob_vectors, copy.deepcopy(selected_robs)], dim=0)\n",
    "\n",
    "            anchor_set = diff_anchor_set[:n_phis]\n",
    "            \n",
    "    else:\n",
    "        anchor_set = sampler.bag_sample(bag_size=n_phis, nvars=n_vars) \n",
    "\n",
    "    return anchor_set\n",
    "    # modify this last instruction with the dump one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27166415-9e1b-4bf7-81dc-4650fe45bfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.And at 0x7f77ba1e5040>,\n",
       " <__main__.And at 0x7f77c93d9670>,\n",
       " <__main__.And at 0x7f77ba102670>,\n",
       " <__main__.Not at 0x7f77ba1d5760>,\n",
       " <__main__.Not at 0x7f77ba1d5370>,\n",
       " <__main__.Until at 0x7f77ba1c08b0>,\n",
       " <__main__.Or at 0x7f77ba1c07f0>,\n",
       " <__main__.Globally at 0x7f77ba1c41f0>,\n",
       " <__main__.Globally at 0x7f77ba1c4f70>,\n",
       " <__main__.Until at 0x7f77ba1c4f10>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchorGeneration(anchor_path = 'anchor_set_3_vars.pickle',\n",
    "                 diff_init = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412fe03b-f5e9-484a-a922-9badab11c8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2b1ef-79fd-4a2f-8212-2c765f51e396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
