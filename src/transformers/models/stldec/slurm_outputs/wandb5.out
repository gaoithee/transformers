Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
Running on  nodes
---------------------------------------------
SLURM job ID:        12166085
SLURM job node list: lrdn3217
DATE:                Thu Feb  6 10:20:55 CET 2025
---------------------------------------------
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
02/06/2025 10:21:30 - INFO - __main__ - ***** Running training *****
02/06/2025 10:21:30 - INFO - __main__ -   Num examples = 78773
02/06/2025 10:21:30 - INFO - __main__ -   Num Epochs = 10
02/06/2025 10:21:30 - INFO - __main__ -   Instantaneous batch size per device = 32
02/06/2025 10:21:30 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
02/06/2025 10:21:30 - INFO - __main__ -   Gradient Accumulation steps = 1
02/06/2025 10:21:30 - INFO - __main__ -   Max optimization steps = 50000
  0%|          | 0/50000 [00:00<?, ?it/s]02/06/2025 10:21:30 - INFO - accelerate.accelerator - Loading states from tf_output_test_16batch/step_12000
02/06/2025 10:21:32 - INFO - accelerate.checkpointing - All model weights loaded successfully
02/06/2025 10:21:34 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
02/06/2025 10:21:34 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
02/06/2025 10:21:34 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
02/06/2025 10:21:34 - INFO - accelerate.checkpointing - All random states loaded successfully
02/06/2025 10:21:34 - INFO - accelerate.accelerator - Loading in 0 custom states
02/06/2025 10:21:34 - INFO - __main__ - Starting epoch = 4, resume step = 12000
02/06/2025 10:21:34 - INFO - __main__ - Resuming training from the specified step
02/06/2025 10:21:34 - INFO - __main__ - Total expected steps: 3100
02/06/2025 10:22:21 - INFO - __main__ -   Loss = 0.11592455208301544, epoch = 4, step = 2152
  0%|          | 1/50000 [00:50<696:42:57, 50.16s/it]02/06/2025 10:22:43 - INFO - __main__ -   Loss = 0.13177739083766937, epoch = 4, step = 2153
  0%|          | 2/50000 [01:12<467:57:21, 33.69s/it]02/06/2025 10:23:05 - INFO - __main__ -   Loss = 0.17166516184806824, epoch = 4, step = 2154
  0%|          | 3/50000 [01:34<394:51:17, 28.43s/it]02/06/2025 10:23:27 - INFO - __main__ -   Loss = 0.12336838990449905, epoch = 4, step = 2155
  0%|          | 4/50000 [01:56<360:43:39, 25.97s/it]02/06/2025 10:23:49 - INFO - __main__ -   Loss = 0.15698553621768951, epoch = 4, step = 2156
  0%|          | 5/50000 [02:18<341:42:44, 24.61s/it]02/06/2025 10:24:11 - INFO - __main__ -   Loss = 0.1812031865119934, epoch = 4, step = 2157
  0%|          | 6/50000 [02:41<330:11:47, 23.78s/it]02/06/2025 10:24:34 - INFO - __main__ -   Loss = 0.14123186469078064, epoch = 4, step = 2158
  0%|          | 7/50000 [03:03<323:02:37, 23.26s/it]02/06/2025 10:24:56 - INFO - __main__ -   Loss = 0.17513155937194824, epoch = 4, step = 2159
  0%|          | 8/50000 [03:25<318:18:55, 22.92s/it]02/06/2025 10:25:18 - INFO - __main__ -   Loss = 0.13662414252758026, epoch = 4, step = 2160
  0%|          | 9/50000 [03:47<315:01:16, 22.69s/it]02/06/2025 10:25:40 - INFO - __main__ -   Loss = 0.1565544456243515, epoch = 4, step = 2161
  0%|          | 10/50000 [04:09<312:50:45, 22.53s/it]02/06/2025 10:26:02 - INFO - __main__ -   Loss = 0.17262642085552216, epoch = 4, step = 2162
  0%|          | 11/50000 [04:31<311:20:51, 22.42s/it]02/06/2025 10:26:25 - INFO - __main__ -   Loss = 0.10887881368398666, epoch = 4, step = 2163
  0%|          | 12/50000 [04:54<310:20:01, 22.35s/it]02/06/2025 10:26:47 - INFO - __main__ -   Loss = 0.14497046172618866, epoch = 4, step = 2164
  0%|          | 13/50000 [05:16<309:39:10, 22.30s/it]02/06/2025 10:27:09 - INFO - __main__ -   Loss = 0.15470074117183685, epoch = 4, step = 2165
  0%|          | 14/50000 [05:38<309:06:09, 22.26s/it]02/06/2025 10:27:31 - INFO - __main__ -   Loss = 0.12016334384679794, epoch = 4, step = 2166
  0%|          | 15/50000 [06:00<308:44:09, 22.24s/it]02/06/2025 10:27:53 - INFO - __main__ -   Loss = 0.14892570674419403, epoch = 4, step = 2167
  0%|          | 16/50000 [06:22<308:43:05, 22.23s/it]02/06/2025 10:28:16 - INFO - __main__ -   Loss = 0.11333220452070236, epoch = 4, step = 2168
  0%|          | 17/50000 [06:45<308:32:48, 22.22s/it]02/06/2025 10:28:38 - INFO - __main__ -   Loss = 0.15413613617420197, epoch = 4, step = 2169
  0%|          | 18/50000 [07:07<308:21:33, 22.21s/it]02/06/2025 10:29:00 - INFO - __main__ -   Loss = 0.15045495331287384, epoch = 4, step = 2170
  0%|          | 19/50000 [07:29<308:12:15, 22.20s/it]02/06/2025 10:29:22 - INFO - __main__ -   Loss = 0.15494735538959503, epoch = 4, step = 2171
  0%|          | 20/50000 [07:51<308:07:52, 22.19s/it]02/06/2025 10:29:44 - INFO - __main__ -   Loss = 0.12963201105594635, epoch = 4, step = 2172
  0%|          | 21/50000 [08:13<307:56:10, 22.18s/it]02/06/2025 10:30:06 - INFO - __main__ -   Loss = 0.14450518786907196, epoch = 4, step = 2173
  0%|          | 22/50000 [08:36<308:02:16, 22.19s/it]02/06/2025 10:30:29 - INFO - __main__ -   Loss = 0.1726153939962387, epoch = 4, step = 2174
  0%|          | 23/50000 [08:58<308:10:39, 22.20s/it]02/06/2025 10:30:51 - INFO - __main__ -   Loss = 0.16032834351062775, epoch = 4, step = 2175
  0%|          | 24/50000 [09:20<308:14:29, 22.20s/it]02/06/2025 10:31:13 - INFO - __main__ -   Loss = 0.15888376533985138, epoch = 4, step = 2176
  0%|          | 25/50000 [09:42<308:11:42, 22.20s/it]02/06/2025 10:31:35 - INFO - __main__ -   Loss = 0.1236584410071373, epoch = 4, step = 2177
  0%|          | 26/50000 [10:04<308:12:24, 22.20s/it]02/06/2025 10:31:57 - INFO - __main__ -   Loss = 0.11916600167751312, epoch = 4, step = 2178
  0%|          | 27/50000 [10:27<308:07:54, 22.20s/it]02/06/2025 10:32:20 - INFO - __main__ -   Loss = 0.13738037645816803, epoch = 4, step = 2179
  0%|          | 28/50000 [10:49<308:11:54, 22.20s/it]02/06/2025 10:32:42 - INFO - __main__ -   Loss = 0.17122282087802887, epoch = 4, step = 2180
  0%|          | 29/50000 [11:11<308:04:14, 22.19s/it]02/06/2025 10:33:04 - INFO - __main__ -   Loss = 0.1187075600028038, epoch = 4, step = 2181
  0%|          | 30/50000 [11:33<308:08:30, 22.20s/it]02/06/2025 10:33:26 - INFO - __main__ -   Loss = 0.12194493412971497, epoch = 4, step = 2182
  0%|          | 31/50000 [11:55<308:17:28, 22.21s/it]02/06/2025 10:33:49 - INFO - __main__ -   Loss = 0.15236039459705353, epoch = 4, step = 2183
