Loading python/3.11.6--gcc--8.5.0
  Loading requirement: bzip2/1.0.8-gp5wcz5 libmd/1.0.4-wja3f5q
    libbsd/0.11.7-cgxjopl expat/2.5.0-bptl3xw ncurses/6.4-asx3jea
    readline/8.2-nyw6mp6 gdbm/1.23-fs6otck libiconv/1.17-d7yvx2s
    xz/5.4.1-hubmwr5 zlib-ng/2.1.4-6htiapk libxml2/2.10.3-5eeeokp
    pigz/2.7-bopr5vp zstd/1.5.5-gawytfl tar/1.34-amqus5s gettext/0.22.3-2g7elif
    libffi/3.4.4-6r7brdq libxcrypt/4.4.35-ss2rzin sqlite/3.43.2
    util-linux-uuid/2.38.1-jkdi7kv
Running on  nodes
---------------------------------------------
SLURM job ID:        12479073
SLURM job node list: lrdn3186
DATE:                Fri Feb 14 14:58:50 CET 2025
---------------------------------------------
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
02/14/2025 14:59:25 - INFO - __main__ - ***** Running training *****
02/14/2025 14:59:25 - INFO - __main__ -   Num examples = 78773
02/14/2025 14:59:25 - INFO - __main__ -   Num Epochs = 10
02/14/2025 14:59:25 - INFO - __main__ -   Instantaneous batch size per device = 32
02/14/2025 14:59:25 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32
02/14/2025 14:59:25 - INFO - __main__ -   Gradient Accumulation steps = 1
02/14/2025 14:59:25 - INFO - __main__ -   Max optimization steps = 50000
  0%|          | 0/50000 [00:00<?, ?it/s]02/14/2025 14:59:25 - INFO - accelerate.accelerator - Loading states from tf_output_test_16batch/step_20000
02/14/2025 14:59:27 - INFO - accelerate.checkpointing - All model weights loaded successfully
02/14/2025 14:59:29 - INFO - accelerate.checkpointing - All optimizer states loaded successfully
02/14/2025 14:59:29 - INFO - accelerate.checkpointing - All scheduler states loaded successfully
02/14/2025 14:59:29 - INFO - accelerate.checkpointing - All dataloader sampler states loaded successfully
02/14/2025 14:59:29 - INFO - accelerate.checkpointing - All random states loaded successfully
02/14/2025 14:59:29 - INFO - accelerate.accelerator - Loading in 0 custom states
02/14/2025 14:59:29 - INFO - __main__ - Starting epoch = 7, resume step = 20000
02/14/2025 14:59:29 - INFO - __main__ - Resuming training from the specified step
02/14/2025 14:59:29 - INFO - __main__ - Total expected steps: 21580
02/14/2025 15:00:12 - INFO - __main__ -   Loss = 0.13293462991714478, epoch = 8, step = 304
  0%|          | 1/50000 [00:46<650:27:27, 46.83s/it]02/14/2025 15:00:12 - INFO - accelerate.accelerator - Saving current state to epoch_8/epoch_8
02/14/2025 15:00:12 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/14/2025 15:00:15 - INFO - accelerate.checkpointing - Model weights saved in epoch_8/epoch_8/model.safetensors
02/14/2025 15:00:44 - INFO - accelerate.checkpointing - Optimizer state saved in epoch_8/epoch_8/optimizer.bin
02/14/2025 15:00:44 - INFO - accelerate.checkpointing - Scheduler state saved in epoch_8/epoch_8/scheduler.bin
02/14/2025 15:00:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in epoch_8/epoch_8/sampler.bin
02/14/2025 15:00:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in epoch_8/epoch_8/sampler_1.bin
02/14/2025 15:00:44 - INFO - accelerate.checkpointing - Random states saved in epoch_8/epoch_8/random_states_0.pkl
02/14/2025 15:01:04 - INFO - __main__ -   Loss = 0.1459471732378006, epoch = 8, step = 305
  0%|          | 2/50000 [01:38<693:45:34, 49.95s/it]02/14/2025 15:01:04 - INFO - accelerate.accelerator - Saving current state to epoch_8/epoch_8
02/14/2025 15:01:04 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/14/2025 15:01:10 - INFO - accelerate.checkpointing - Model weights saved in epoch_8/epoch_8/model.safetensors
02/14/2025 15:01:45 - INFO - accelerate.checkpointing - Optimizer state saved in epoch_8/epoch_8/optimizer.bin
02/14/2025 15:01:45 - INFO - accelerate.checkpointing - Scheduler state saved in epoch_8/epoch_8/scheduler.bin
02/14/2025 15:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in epoch_8/epoch_8/sampler.bin
02/14/2025 15:01:45 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in epoch_8/epoch_8/sampler_1.bin
02/14/2025 15:01:45 - INFO - accelerate.checkpointing - Random states saved in epoch_8/epoch_8/random_states_0.pkl
02/14/2025 15:02:06 - INFO - __main__ -   Loss = 0.19058646261692047, epoch = 8, step = 306
  0%|          | 3/50000 [02:40<769:41:47, 55.42s/it]02/14/2025 15:02:06 - INFO - accelerate.accelerator - Saving current state to epoch_8/epoch_8
02/14/2025 15:02:06 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/14/2025 15:02:09 - INFO - accelerate.checkpointing - Model weights saved in epoch_8/epoch_8/model.safetensors
02/14/2025 15:02:46 - INFO - accelerate.checkpointing - Optimizer state saved in epoch_8/epoch_8/optimizer.bin
02/14/2025 15:02:46 - INFO - accelerate.checkpointing - Scheduler state saved in epoch_8/epoch_8/scheduler.bin
02/14/2025 15:02:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in epoch_8/epoch_8/sampler.bin
02/14/2025 15:02:46 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in epoch_8/epoch_8/sampler_1.bin
02/14/2025 15:02:46 - INFO - accelerate.checkpointing - Random states saved in epoch_8/epoch_8/random_states_0.pkl
02/14/2025 15:03:07 - INFO - __main__ -   Loss = 0.1300979107618332, epoch = 8, step = 307
  0%|          | 4/50000 [03:41<797:18:00, 57.41s/it]02/14/2025 15:03:07 - INFO - accelerate.accelerator - Saving current state to epoch_8/epoch_8
02/14/2025 15:03:07 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/14/2025 15:03:11 - INFO - accelerate.checkpointing - Model weights saved in epoch_8/epoch_8/model.safetensors
02/14/2025 15:04:02 - INFO - accelerate.checkpointing - Optimizer state saved in epoch_8/epoch_8/optimizer.bin
02/14/2025 15:04:02 - INFO - accelerate.checkpointing - Scheduler state saved in epoch_8/epoch_8/scheduler.bin
02/14/2025 15:04:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in epoch_8/epoch_8/sampler.bin
02/14/2025 15:04:02 - INFO - accelerate.checkpointing - Sampler state for dataloader 1 saved in epoch_8/epoch_8/sampler_1.bin
02/14/2025 15:04:02 - INFO - accelerate.checkpointing - Random states saved in epoch_8/epoch_8/random_states_0.pkl
02/14/2025 15:04:23 - INFO - __main__ -   Loss = 0.10409903526306152, epoch = 8, step = 308
  0%|          | 5/50000 [04:57<893:22:04, 64.33s/it]02/14/2025 15:04:23 - INFO - accelerate.accelerator - Saving current state to epoch_8/epoch_8
02/14/2025 15:04:23 - WARNING - accelerate.utils.other - Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading
02/14/2025 15:04:53 - INFO - accelerate.checkpointing - Model weights saved in epoch_8/epoch_8/model.safetensors
srun: Job step aborted: Waiting up to 92 seconds for job step to finish.
slurmstepd: error: *** JOB 12479073 ON lrdn3186 CANCELLED AT 2025-02-14T15:05:18 ***
slurmstepd: error: *** STEP 12479073.0 ON lrdn3186 CANCELLED AT 2025-02-14T15:05:18 ***
